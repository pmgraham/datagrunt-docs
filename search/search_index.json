{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Datagrunt! Datagrunt is a lightweight CSV processing library that simplifies handling CSV files and converting them into various formats such as Excel, JSON, JSON Newline Delimited, and Parquet. It eliminates repetitive coding tasks when working with CSVs, empowering data analysts, engineers, and scientists to focus on insights rather than tedious data wrangling. Key Features Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for CSV files using a custom algorithm powered by DuckDB. Seamless Data Processing: Leverage DuckDB and Polars to perform data processing tasks directly on CSV data. Flexible Transformation: Easily convert processed CSV data into different formats to meet your needs. Pythonic API: Enjoy a clean and intuitive API that seamlessly integrates into existing Python workflows. About DuckDB and Polars Processing CSV files, especially large ones locally or in resource-constrained data pipelines, can be challenging. Python, while powerful, is not known for its speed in heavy data processing tasks. DuckDB and Polars address this limitation. DuckDB is an extremely fast in-process analytical database that's easy to use with Python through its API. It provides a simple interface for processing CSV files and offers powerful local analytics capabilities. Polars is a multi-threaded query engine written in Rust, known for its efficient parallelism and high performance on modern processors. Both DuckDB are easy to use and open source. What Datagrunt Is Not Datagrunt is not an extension of DuckDB or Polars, nor is it designed to handle every data processing task. Its core purpose is to accurately infer CSV delimiters and expose them to classes that offer helper methods for common data tasks. Datagrunt helps load CSV files into Polars dataframes or convert them into new formats. It's not a replacement for DuckDB, Pandas, or Polars. Complete Choice and Freedom Datagrunt doesn't restrict you to the features exposed through its library. By default, it loads data into Polars dataframes when returning objects in memory, but you can convert them into Pandas dataframes using the to_pandas method. While some users may only need Datagrunt for delimiter inference, it provides helper classes and methods that can be used as desired. You can always leverage Datagrunt's output in other contexts within the same script or application.","title":"Home"},{"location":"#welcome-to-datagrunt","text":"Datagrunt is a lightweight CSV processing library that simplifies handling CSV files and converting them into various formats such as Excel, JSON, JSON Newline Delimited, and Parquet. It eliminates repetitive coding tasks when working with CSVs, empowering data analysts, engineers, and scientists to focus on insights rather than tedious data wrangling.","title":"Welcome to Datagrunt!"},{"location":"#key-features","text":"Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for CSV files using a custom algorithm powered by DuckDB. Seamless Data Processing: Leverage DuckDB and Polars to perform data processing tasks directly on CSV data. Flexible Transformation: Easily convert processed CSV data into different formats to meet your needs. Pythonic API: Enjoy a clean and intuitive API that seamlessly integrates into existing Python workflows.","title":"Key Features"},{"location":"#about-duckdb-and-polars","text":"Processing CSV files, especially large ones locally or in resource-constrained data pipelines, can be challenging. Python, while powerful, is not known for its speed in heavy data processing tasks. DuckDB and Polars address this limitation. DuckDB is an extremely fast in-process analytical database that's easy to use with Python through its API. It provides a simple interface for processing CSV files and offers powerful local analytics capabilities. Polars is a multi-threaded query engine written in Rust, known for its efficient parallelism and high performance on modern processors. Both DuckDB are easy to use and open source.","title":"About DuckDB and Polars"},{"location":"#what-datagrunt-is-not","text":"Datagrunt is not an extension of DuckDB or Polars, nor is it designed to handle every data processing task. Its core purpose is to accurately infer CSV delimiters and expose them to classes that offer helper methods for common data tasks. Datagrunt helps load CSV files into Polars dataframes or convert them into new formats. It's not a replacement for DuckDB, Pandas, or Polars.","title":"What Datagrunt Is Not"},{"location":"#complete-choice-and-freedom","text":"Datagrunt doesn't restrict you to the features exposed through its library. By default, it loads data into Polars dataframes when returning objects in memory, but you can convert them into Pandas dataframes using the to_pandas method. While some users may only need Datagrunt for delimiter inference, it provides helper classes and methods that can be used as desired. You can always leverage Datagrunt's output in other contexts within the same script or application.","title":"Complete Choice and Freedom"},{"location":"api/","text":"API Documentation Core Classes FileProperties Description : A base class providing essential properties and methods for working with files. Properties filepath (str): The full path to the file. filename (str): The name of the file, without the directory path. extension (str): The file extension, including the leading dot (e.g., \".csv\"). extension_string (str): The file extension without the leading dot (e.g., \"csv\"). size_in_bytes (int): The size of the file in bytes. size_in_kb (float): The size of the file in kilobytes. size_in_mb (float): The size of the file in megabytes. size_in_gb (float): The size of the file in gigabytes. size_in_tb (float): The size of the file in terabytes. is_structured (bool): True if the file is considered structured (based on extension). is_semi_structured (bool): True if the file is considered semi-structured (based on extension). is_unstructured (bool): True if the file is considered unstructured (based on extension). is_standard (bool): True if the file is a standard file type (based on extension). is_proprietary (bool): True if the file is a proprietary file type (based on extension). is_csv (bool): True if the file is a CSV file (based on extension). is_excel (bool): True if the file is an Excel file (based on extension). is_apache (bool): True if the file is an Apache file (like Parquet or Avro) (based on extension). is_empty (bool): True if the file size is 0 bytes. is_large (bool): True if the file size is 1 GB or larger. is_tabular (bool): True if the file is a tabular data format (CSV, Excel, TSV). CSVProperties Description: Extends FileProperties with properties and methods specific to CSV files. Properties first_row (str): The content of the first row in the CSV file. delimiter (str): The delimiter used to separate values in the CSV file (inferred). row_count_with_header (int): The total number of rows in the CSV file, including the header row. row_count_without_header (int): The total number of rows in the CSV file, excluding the header row. columns (list): A list of column names from the CSV file's header. columns_string (str): A comma-separated string of column names. columns_byte_string (bytes): A byte string representation of the comma-separated column names. column_count (int): The number of columns in the CSV file. quotechar (str): The character used for quoting fields in the CSV file. escapechar (str): The character used for escaping characters in the CSV file. newline_delimiter (str): The character(s) used to represent a newline in the CSV file. CSV Reader and Writer Classes Properties All of the CSVReader and CSVWriter classes in this section extends the CSVProperties class. CSVReaderDuckDBEngine Description: Reads CSV files and converts them into various in-memory data objects using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVReaderDuckDBEngine class. Instance Methods get_sample() Description: Prints a sample of the CSV file to the console, providing a glimpse into its contents and structure. Parameters: None Returns: None to_dataframe() Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data. to_arrow_table() Description: Reads the CSV file and converts it into a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data. to_dicts() Description: Reads the CSV file and converts it into a list of dictionaries, where each dictionary represents a row in the CSV. Parameters: None Returns: A list of dictionaries representing the CSV data. CSVReaderPolarsEngine Description: Reads CSV files and converts them into various in-memory data objects using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted. get_sample() Description: Reads the CSV file and returns a sample DataFrame with a limited number of rows (defined by DATAFRAME_SAMPLE_ROWS). Parameters: None Returns: A Polars DataFrame containing a sample of the CSV data. to_dataframe() Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data. to_arrow_table() Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data. to_dicts() Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a list of dictionaries. Parameters: None Returns: A list of dictionaries representing the CSV data. CSVWriterDuckDBEngine Description: Converts CSV files to various other supported file types using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVWriterDuckDBEngine class. Instance Methods write_csv() Description: Reads the CSV file using DuckDB and writes it to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used. write_excel() Description: Reads the CSV file using DuckDB and writes it to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used. write_json() Description: Reads the CSV file using DuckDB and writes it to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used. write_json_newline_delimited() Description: Reads the CSV file using DuckDB and writes it to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used. write_parquet() Description: Reads the CSV file using DuckDB and writes it to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used. CSVWriterPolarsEngine Description: Writes data from various sources to CSV files using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Instance Methods write_csv() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used. write_excel() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used. write_json() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used. write_json_newline_delimited() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used. write_parquet() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used.","title":"API"},{"location":"api/#api-documentation","text":"","title":"API Documentation"},{"location":"api/#core-classes","text":"","title":"Core Classes"},{"location":"api/#fileproperties","text":"Description : A base class providing essential properties and methods for working with files. Properties filepath (str): The full path to the file. filename (str): The name of the file, without the directory path. extension (str): The file extension, including the leading dot (e.g., \".csv\"). extension_string (str): The file extension without the leading dot (e.g., \"csv\"). size_in_bytes (int): The size of the file in bytes. size_in_kb (float): The size of the file in kilobytes. size_in_mb (float): The size of the file in megabytes. size_in_gb (float): The size of the file in gigabytes. size_in_tb (float): The size of the file in terabytes. is_structured (bool): True if the file is considered structured (based on extension). is_semi_structured (bool): True if the file is considered semi-structured (based on extension). is_unstructured (bool): True if the file is considered unstructured (based on extension). is_standard (bool): True if the file is a standard file type (based on extension). is_proprietary (bool): True if the file is a proprietary file type (based on extension). is_csv (bool): True if the file is a CSV file (based on extension). is_excel (bool): True if the file is an Excel file (based on extension). is_apache (bool): True if the file is an Apache file (like Parquet or Avro) (based on extension). is_empty (bool): True if the file size is 0 bytes. is_large (bool): True if the file size is 1 GB or larger. is_tabular (bool): True if the file is a tabular data format (CSV, Excel, TSV).","title":"FileProperties"},{"location":"api/#csvproperties","text":"Description: Extends FileProperties with properties and methods specific to CSV files. Properties first_row (str): The content of the first row in the CSV file. delimiter (str): The delimiter used to separate values in the CSV file (inferred). row_count_with_header (int): The total number of rows in the CSV file, including the header row. row_count_without_header (int): The total number of rows in the CSV file, excluding the header row. columns (list): A list of column names from the CSV file's header. columns_string (str): A comma-separated string of column names. columns_byte_string (bytes): A byte string representation of the comma-separated column names. column_count (int): The number of columns in the CSV file. quotechar (str): The character used for quoting fields in the CSV file. escapechar (str): The character used for escaping characters in the CSV file. newline_delimiter (str): The character(s) used to represent a newline in the CSV file.","title":"CSVProperties"},{"location":"api/#csv-reader-and-writer-classes","text":"","title":"CSV Reader and Writer Classes"},{"location":"api/#properties","text":"All of the CSVReader and CSVWriter classes in this section extends the CSVProperties class.","title":"Properties"},{"location":"api/#csvreaderduckdbengine","text":"Description: Reads CSV files and converts them into various in-memory data objects using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVReaderDuckDBEngine class. Instance Methods","title":"CSVReaderDuckDBEngine"},{"location":"api/#get_sample","text":"Description: Prints a sample of the CSV file to the console, providing a glimpse into its contents and structure. Parameters: None Returns: None","title":"get_sample()"},{"location":"api/#to_dataframe","text":"Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data.","title":"to_dataframe()"},{"location":"api/#to_arrow_table","text":"Description: Reads the CSV file and converts it into a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data.","title":"to_arrow_table()"},{"location":"api/#to_dicts","text":"Description: Reads the CSV file and converts it into a list of dictionaries, where each dictionary represents a row in the CSV. Parameters: None Returns: A list of dictionaries representing the CSV data.","title":"to_dicts()"},{"location":"api/#csvreaderpolarsengine","text":"Description: Reads CSV files and converts them into various in-memory data objects using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted.","title":"CSVReaderPolarsEngine"},{"location":"api/#get_sample_1","text":"Description: Reads the CSV file and returns a sample DataFrame with a limited number of rows (defined by DATAFRAME_SAMPLE_ROWS). Parameters: None Returns: A Polars DataFrame containing a sample of the CSV data.","title":"get_sample()"},{"location":"api/#to_dataframe_1","text":"Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data.","title":"to_dataframe()"},{"location":"api/#to_arrow_table_1","text":"Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data.","title":"to_arrow_table()"},{"location":"api/#to_dicts_1","text":"Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a list of dictionaries. Parameters: None Returns: A list of dictionaries representing the CSV data.","title":"to_dicts()"},{"location":"api/#csvwriterduckdbengine","text":"Description: Converts CSV files to various other supported file types using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVWriterDuckDBEngine class. Instance Methods","title":"CSVWriterDuckDBEngine"},{"location":"api/#write_csv","text":"Description: Reads the CSV file using DuckDB and writes it to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used.","title":"write_csv()"},{"location":"api/#write_excel","text":"Description: Reads the CSV file using DuckDB and writes it to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used.","title":"write_excel()"},{"location":"api/#write_json","text":"Description: Reads the CSV file using DuckDB and writes it to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used.","title":"write_json()"},{"location":"api/#write_json_newline_delimited","text":"Description: Reads the CSV file using DuckDB and writes it to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used.","title":"write_json_newline_delimited()"},{"location":"api/#write_parquet","text":"Description: Reads the CSV file using DuckDB and writes it to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used.","title":"write_parquet()"},{"location":"api/#csvwriterpolarsengine","text":"Description: Writes data from various sources to CSV files using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Instance Methods","title":"CSVWriterPolarsEngine"},{"location":"api/#write_csv_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used.","title":"write_csv()"},{"location":"api/#write_excel_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used.","title":"write_excel()"},{"location":"api/#write_json_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used.","title":"write_json()"},{"location":"api/#write_json_newline_delimited_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used.","title":"write_json_newline_delimited()"},{"location":"api/#write_parquet_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used.","title":"write_parquet()"},{"location":"examples/","text":"Usage Examples Get CSV Delimiter from datagrunt.core.fileproperties import CSVProperties csv_file = 'myfile.csv' dg = CSVProperties(csv_file) dg.delimiter # optionally call .encode() on the delimiter to return a bytestring Manually Set CSV Delimiter from datagrunt.core.fileproperties import CSVProperties csv_file = 'myfile.csv' dg = CSVProperties(csv_file) dg.delimiter = ',' # override the inferred delimiter Sample CSV from datagrunt.csvfile import CSVReaderDuckDBEngine, CSVReaderPolarsEngine csv_file = 'myfile.csv' # Sample using DuckDB CSVReaderDuckDBEngine(csv_file).get_sample() # automatically prints to screen # Samole using Polars CSVReaderPolarsEngine(csv_file).get_sample() # automatically prints to screen Read CSV File DuckDB from datagrunt.csvfile import CSVReaderDuckDBEngine csv_file = 'myfile.csv' dg = CSVReaderDuckDBEngine(csv_file) df = dg.to_dataframe() Read CSV File Polars from datagrunt.csvfile import CSVReaderPolarsEngine csv_file = 'myfile.csv' dg = CSVReaderPolarsEngine(csv_file) df = dg.to_dataframe() Write CSV To JSON DuckDB from datagrunt.csvfile import CSVWriterDuckDBEngine csv_file = 'myfile.csv' dg = CSVWriterDuckDBEngine(csv_file) # you don't have to load a reader of all you need to do is convert the file dg.write_json() # you may optionally pass in a filename ### Writefrom CSV To JSON Lines DuckDB dg.write_json_newline_delimited() # if you pass in a filename, ensure it ends with .jsonl Write CSV To JSON Polars from datagrunt.csvfile import CSVWriterPolarsEngine csv_file = 'myfile.csv' dg = CSVReaderPolarsEngine(csv_file) dg.write_json() # you may optionally pass in a filename ### Writefrom CSV To JSON Lines DuckDB dg.write_json_newline_delimited() # if you pass in a filename, ensure it ends with .jsonl Datagrunt With Pandas import pandas as pd from datagrunt.core.fileproperties import CSVProperties csv_file = 'myfile.csv' dg_delimiter = CSVProperties(csv_file).delimiter df = pd.read_csv(csv_file, delimiter=dg_delimiter)","title":"Usage Examples"},{"location":"examples/#usage-examples","text":"","title":"Usage Examples"},{"location":"examples/#get-csv-delimiter","text":"from datagrunt.core.fileproperties import CSVProperties csv_file = 'myfile.csv' dg = CSVProperties(csv_file) dg.delimiter # optionally call .encode() on the delimiter to return a bytestring","title":"Get CSV Delimiter"},{"location":"examples/#manually-set-csv-delimiter","text":"from datagrunt.core.fileproperties import CSVProperties csv_file = 'myfile.csv' dg = CSVProperties(csv_file) dg.delimiter = ',' # override the inferred delimiter","title":"Manually Set CSV Delimiter"},{"location":"examples/#sample-csv","text":"from datagrunt.csvfile import CSVReaderDuckDBEngine, CSVReaderPolarsEngine csv_file = 'myfile.csv' # Sample using DuckDB CSVReaderDuckDBEngine(csv_file).get_sample() # automatically prints to screen # Samole using Polars CSVReaderPolarsEngine(csv_file).get_sample() # automatically prints to screen","title":"Sample CSV"},{"location":"examples/#read-csv-file-duckdb","text":"from datagrunt.csvfile import CSVReaderDuckDBEngine csv_file = 'myfile.csv' dg = CSVReaderDuckDBEngine(csv_file) df = dg.to_dataframe()","title":"Read CSV File DuckDB"},{"location":"examples/#read-csv-file-polars","text":"from datagrunt.csvfile import CSVReaderPolarsEngine csv_file = 'myfile.csv' dg = CSVReaderPolarsEngine(csv_file) df = dg.to_dataframe()","title":"Read CSV File Polars"},{"location":"examples/#write-csv-to-json-duckdb","text":"from datagrunt.csvfile import CSVWriterDuckDBEngine csv_file = 'myfile.csv' dg = CSVWriterDuckDBEngine(csv_file) # you don't have to load a reader of all you need to do is convert the file dg.write_json() # you may optionally pass in a filename ### Writefrom CSV To JSON Lines DuckDB dg.write_json_newline_delimited() # if you pass in a filename, ensure it ends with .jsonl","title":"Write CSV To JSON DuckDB"},{"location":"examples/#write-csv-to-json-polars","text":"from datagrunt.csvfile import CSVWriterPolarsEngine csv_file = 'myfile.csv' dg = CSVReaderPolarsEngine(csv_file) dg.write_json() # you may optionally pass in a filename ### Writefrom CSV To JSON Lines DuckDB dg.write_json_newline_delimited() # if you pass in a filename, ensure it ends with .jsonl","title":"Write CSV To JSON Polars"},{"location":"examples/#datagrunt-with-pandas","text":"import pandas as pd from datagrunt.core.fileproperties import CSVProperties csv_file = 'myfile.csv' dg_delimiter = CSVProperties(csv_file).delimiter df = pd.read_csv(csv_file, delimiter=dg_delimiter)","title":"Datagrunt With Pandas"},{"location":"guidance/","text":"General Guidance Overview As you'll see in the API section of the documentation, there are four main classes to use for reading and writing CSV files: CSVReaderDuckDBEngine CSVReaderPolarsEngine CSVWriterDuckDBEngine CSVWriterPolarsEngine The reason options exist with DuckDB and Polars is that generally speaking each of them perform a bit better than the other depending on the circumstances. For example, Polars is extremely efficient at reading CSVs up to a certain size. After a certain point with larger files DuckDB tends to perform better. During testing when Datagrunt was first created, CSV files smaller than 26 million records loaded into a Polars dataframe in under 4 seconds in most cases. DuckDB, reading the exact same file, could load the data in about three and a half seconds, but it took another second and a half to make the data accessible. At this scale you're only talking a couple of seconds but to some that might make a huge difference for their use case. However, when it comes to converting CSV files to JSON, for example, DuckDB was not only faster than Polars on a 26 million record CSV file, it also wrote JSON that was very clean. When testing with Polars, there were occassions where the CSV file that was converted to JSON from a Polars dataframe wrote to an incorrectly formatted JSON file. DuckDB encountered no such issue and was consistently faster and had cleaner output on writes. The other reason DuckDB is given as an option is that once the CSV file is impoted into the local database, you can then execute SQL queries against that data at tremendous speed. This same 26 million record dataset that's been referred to a couple of times imported the CSV file and executed an aggregate query in about four seconds or less on average. That kind of speed and power is not only powerful for analytics, it enables ETL at great speed with processing files. Finally, you do not have to use any of the Datagrunt helpers if you wish to use other options. You may simply use Datagrunt to infer the CSV file delimiter, assign that delimiter to a variable, and then process your CSV files however you wish with whatever process or library you desire. In conclusion, DuckDB and Polars are both presented as options in order to give the user the choice and the opportunity to use each engine in the use situation where you can get the best performance under the circumstances. A Note About Pandas Pandas is an amazing Python library and has served the community extremely well over the years. However, as you can see from the Polars website, Polars is exponentially faster than Pandas in many (if not most) use cases. Therefore, the decision was made to use Polars for dataframes in Datagrunt. A Note About Spark Like Pandas, Spark has served the community well. However, it is the opinion of the Datagrunt team that the combination of DuckDB and Polars presents an opportunity to reduce our dependence on the Spark ecosystem for batch processing. Again, as you can see on the Polars website, Polars is significantly faster than Spark for data processing in many situations. Not only is Polars less complex to use than Spark, but Polars is incredibly efficient and fast when using machines with smaller compute footprints. Ideal Scenarios For Datagrunt Datagrunt is ideally utilized during exploratory data analysis, data engineering pipelines, or data science workloads that depend on CSV files. With the very small compute footprint required from both DuckDB and Polars, building pipelines in serverless microservices (such as Google's Cloud Run Functions) is easy, efficent, cost effective, and relatively simple to build and maintain. References Kaggle's The Movies Dataset that was used in testing. Specifically, we used the ratings.csv file in most tests. Tests were conducted on a base model 13 inch 2024 MacBook Air M3 .","title":"General Guidance"},{"location":"guidance/#general-guidance","text":"","title":"General Guidance"},{"location":"guidance/#overview","text":"As you'll see in the API section of the documentation, there are four main classes to use for reading and writing CSV files: CSVReaderDuckDBEngine CSVReaderPolarsEngine CSVWriterDuckDBEngine CSVWriterPolarsEngine The reason options exist with DuckDB and Polars is that generally speaking each of them perform a bit better than the other depending on the circumstances. For example, Polars is extremely efficient at reading CSVs up to a certain size. After a certain point with larger files DuckDB tends to perform better. During testing when Datagrunt was first created, CSV files smaller than 26 million records loaded into a Polars dataframe in under 4 seconds in most cases. DuckDB, reading the exact same file, could load the data in about three and a half seconds, but it took another second and a half to make the data accessible. At this scale you're only talking a couple of seconds but to some that might make a huge difference for their use case. However, when it comes to converting CSV files to JSON, for example, DuckDB was not only faster than Polars on a 26 million record CSV file, it also wrote JSON that was very clean. When testing with Polars, there were occassions where the CSV file that was converted to JSON from a Polars dataframe wrote to an incorrectly formatted JSON file. DuckDB encountered no such issue and was consistently faster and had cleaner output on writes. The other reason DuckDB is given as an option is that once the CSV file is impoted into the local database, you can then execute SQL queries against that data at tremendous speed. This same 26 million record dataset that's been referred to a couple of times imported the CSV file and executed an aggregate query in about four seconds or less on average. That kind of speed and power is not only powerful for analytics, it enables ETL at great speed with processing files. Finally, you do not have to use any of the Datagrunt helpers if you wish to use other options. You may simply use Datagrunt to infer the CSV file delimiter, assign that delimiter to a variable, and then process your CSV files however you wish with whatever process or library you desire. In conclusion, DuckDB and Polars are both presented as options in order to give the user the choice and the opportunity to use each engine in the use situation where you can get the best performance under the circumstances.","title":"Overview"},{"location":"guidance/#a-note-about-pandas","text":"Pandas is an amazing Python library and has served the community extremely well over the years. However, as you can see from the Polars website, Polars is exponentially faster than Pandas in many (if not most) use cases. Therefore, the decision was made to use Polars for dataframes in Datagrunt.","title":"A Note About Pandas"},{"location":"guidance/#a-note-about-spark","text":"Like Pandas, Spark has served the community well. However, it is the opinion of the Datagrunt team that the combination of DuckDB and Polars presents an opportunity to reduce our dependence on the Spark ecosystem for batch processing. Again, as you can see on the Polars website, Polars is significantly faster than Spark for data processing in many situations. Not only is Polars less complex to use than Spark, but Polars is incredibly efficient and fast when using machines with smaller compute footprints.","title":"A Note About Spark"},{"location":"guidance/#ideal-scenarios-for-datagrunt","text":"Datagrunt is ideally utilized during exploratory data analysis, data engineering pipelines, or data science workloads that depend on CSV files. With the very small compute footprint required from both DuckDB and Polars, building pipelines in serverless microservices (such as Google's Cloud Run Functions) is easy, efficent, cost effective, and relatively simple to build and maintain.","title":"Ideal Scenarios For Datagrunt"},{"location":"guidance/#references","text":"Kaggle's The Movies Dataset that was used in testing. Specifically, we used the ratings.csv file in most tests. Tests were conducted on a base model 13 inch 2024 MacBook Air M3 .","title":"References"},{"location":"roadmap/","text":"Roadmap Potential Features Currently Datagrunt only focuses on reading CSV files and either using them in memory as dataframes or Python objects such as dictionaries. It's our desire to keep Datagrunt lean and focused on its core use case. However, we do seek to strike a balance between remaining lean and building new features that add value. That said, here are some of the features we are considering: Reading multiple CSV files and writing them to different file formats at scale Change data capture between two CSV files Change data capture between two groups of CSV files Pushing CSV files to Google Cloud Storage Pushing CSV files to BigQuery Pushing CSV files to Postgres Managing schema changes between old files and new files from the same source, representing the same data Genrative AI Currently Datagrunt doesn't use any Generative AI as part of its functionaly. However, we are thinking about use cases where Generative AI could be helpful, specifically in the context of data processing. We will post more information as this idea develops, but we're not going to add Generative AI to Datagunt just to have it in there. If we do add it, it must serve a real purpose that's value-add and not a \"fluff\" feature or set of features. Again, we want to keep Datagrunt lean by focusing on its core use case. Potentail API Managed Service We are considering offering Datagrunt as a managed API service. This would simplify access to Datagrunt specifically for cloud use cases More to come on this as it's only an idea right now. Other Programming Languages Currently there are plans to support only Python in the near future. That said, we are considering porting Datagrunt to Go. More to come on this as things progress.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"roadmap/#potential-features","text":"Currently Datagrunt only focuses on reading CSV files and either using them in memory as dataframes or Python objects such as dictionaries. It's our desire to keep Datagrunt lean and focused on its core use case. However, we do seek to strike a balance between remaining lean and building new features that add value. That said, here are some of the features we are considering: Reading multiple CSV files and writing them to different file formats at scale Change data capture between two CSV files Change data capture between two groups of CSV files Pushing CSV files to Google Cloud Storage Pushing CSV files to BigQuery Pushing CSV files to Postgres Managing schema changes between old files and new files from the same source, representing the same data","title":"Potential Features"},{"location":"roadmap/#genrative-ai","text":"Currently Datagrunt doesn't use any Generative AI as part of its functionaly. However, we are thinking about use cases where Generative AI could be helpful, specifically in the context of data processing. We will post more information as this idea develops, but we're not going to add Generative AI to Datagunt just to have it in there. If we do add it, it must serve a real purpose that's value-add and not a \"fluff\" feature or set of features. Again, we want to keep Datagrunt lean by focusing on its core use case.","title":"Genrative AI"},{"location":"roadmap/#potentail-api-managed-service","text":"We are considering offering Datagrunt as a managed API service. This would simplify access to Datagrunt specifically for cloud use cases More to come on this as it's only an idea right now.","title":"Potentail API Managed Service"},{"location":"roadmap/#other-programming-languages","text":"Currently there are plans to support only Python in the near future. That said, we are considering porting Datagrunt to Go. More to come on this as things progress.","title":"Other Programming Languages"}]}