{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome To Datagrunt Datagrunt is a Python library designed to simplify the way you work with CSV files. It provides a streamlined approach to reading, processing, and transforming your data into various formats, making data manipulation efficient and intuitive. Datagrunt is powered DuckDB , an incredibly fast an flexible locally hosted analytics (OLAP) database. Why Datagrunt? Born out of real-world frustration, Datagrunt eliminates the need for repetitive coding when handling CSV files. Whether you're a data analyst, data engineer, or data scientist, Datagrunt empowers you to focus on insights, not tedious data wrangling. Datagrunt will auto parse CSV files, automatically inferring key attributes such as the file delimiter, all while giving you the necessary level of control to provide your own paramaters as needed (such as setting your own delimiter vs having Datagrunt auto infer the delimiter for you). Key Features Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for your CSV files using a custom algorithm and the power of DuckDB. Seamless Data Processing: Leverage the robust capabilities of DuckDB to perform advanced data processing tasks directly on your CSV data. Flexible Transformation: Easily convert your processed CSV data into various formats to suit your needs. Pythonic API: Enjoy a clean and intuitive API that integrates seamlessly into your existing Python workflows. Why DuckDB? In a nutshell, DuckDB is an extremely fast in-process analytical database. It requires no setup to use with Python and it's fully accessible using its Python API. You can read more about DuckDB here . The reason for building Datagrunt with DuckDB integration is to provide a simple interface for processing CSV files while giving you access to an incredibly power tool for local analytics powered by DuckDB . Why Polars? Polars is a dataframe library written in Rust. According to the Polars website, it's a, \"multi-threaded query engine [that] is written in Rust and designed for effective parallelism. Its vectorized and columnar processing enables cache-coherent algorithms and high performance on modern processors.\" It's also easy to use and open source. By default, all dataframe objects returned by Datagrunt are Polars dataframes. The good news, however, is when you use Datagrunt, you can choose to use Polars or Pandas based on your preference. To convert a Polars dataframe to a Pandas dataframe, simply call to_pandas() as documented here . That said, we'd encourage you to give Polars a try because we think you'll be pleasantly surprised by its performance and simplicity compared to Pandas. Pandas is a wonderful library and we owe a debt of gratitude to Pandas and its creators. However, we feel that in today's world of data that's larger and more complex, dataframe libraries like Polars are more suited to data processing at scale. Ideal Use Cases For Datagrunt Datagrunt is ideal in the following scenarios: exploratory data analysis (EDA) data engineering pipelines converting small and large CSV files into different file formats with incredible speed Installation Get started with Datagrunt in seconds using pip: pip install datagrunt Getting Started from Datagrunt.csvfiles import CSVFile # Load your CSV file csv_file = CSVFile('myfile.csv') # Access file information print(f\"File Size: {csv_file.size_in_mb} MB\") print(f\"CSV Attributes: {csv_file.attributes()}\") # Override inferred delimiter (if needed) csv_file.delimiter = '|' DuckDB Integration for Performant SQL Queries from datagrunt.csvfiles import CSVFile csv_file = CSVFile('myfile.csv') # Construct your SQL query query = f\"\"\" SELECT * FROM {csv_file.duckdb_instance.database_table_name} LIMIT 10 \"\"\" # Execute the query and get results as a Polars DataFrame df = csv_file.select_from_table(query) print(df.head()) Use Datagrunt With the DuckDB Python API from datagrunt import CSVFile import duckdb csv_file = 'myfile.csv' dg = CSVFile(csv_file) # Datagrunt generates all of the file attributes for you automatically. duckdb.read_csv(dg.filepath, delimiter=dg.delimiter) # use the attributes generated by Datagrunt to interface directly with the DuckDB Python API. Contributing We welcome contributions from the community! If you'd like to contribute to Datagrunt, please follow these steps: 1. Fork the repository. 2. Create a new branch for your feature or bug fix. 3. Write tests to cover your changes. 4. Ensure your code passes all tests and follows the project's style guidelines. 5. Submit a pull request for review. Support Roadmap License This project is licensed under the MIT License Acknowledgements A HUGE thank you to the open-source community and the creators of DuckDB and Polars for their fantastic libraries that power Datagrunt.","title":"Home"},{"location":"#welcome-to-datagrunt","text":"Datagrunt is a Python library designed to simplify the way you work with CSV files. It provides a streamlined approach to reading, processing, and transforming your data into various formats, making data manipulation efficient and intuitive. Datagrunt is powered DuckDB , an incredibly fast an flexible locally hosted analytics (OLAP) database.","title":"Welcome To Datagrunt"},{"location":"#why-datagrunt","text":"Born out of real-world frustration, Datagrunt eliminates the need for repetitive coding when handling CSV files. Whether you're a data analyst, data engineer, or data scientist, Datagrunt empowers you to focus on insights, not tedious data wrangling. Datagrunt will auto parse CSV files, automatically inferring key attributes such as the file delimiter, all while giving you the necessary level of control to provide your own paramaters as needed (such as setting your own delimiter vs having Datagrunt auto infer the delimiter for you).","title":"Why Datagrunt?"},{"location":"#key-features","text":"Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for your CSV files using a custom algorithm and the power of DuckDB. Seamless Data Processing: Leverage the robust capabilities of DuckDB to perform advanced data processing tasks directly on your CSV data. Flexible Transformation: Easily convert your processed CSV data into various formats to suit your needs. Pythonic API: Enjoy a clean and intuitive API that integrates seamlessly into your existing Python workflows.","title":"Key Features"},{"location":"#why-duckdb","text":"In a nutshell, DuckDB is an extremely fast in-process analytical database. It requires no setup to use with Python and it's fully accessible using its Python API. You can read more about DuckDB here . The reason for building Datagrunt with DuckDB integration is to provide a simple interface for processing CSV files while giving you access to an incredibly power tool for local analytics powered by DuckDB .","title":"Why DuckDB?"},{"location":"#why-polars","text":"Polars is a dataframe library written in Rust. According to the Polars website, it's a, \"multi-threaded query engine [that] is written in Rust and designed for effective parallelism. Its vectorized and columnar processing enables cache-coherent algorithms and high performance on modern processors.\" It's also easy to use and open source. By default, all dataframe objects returned by Datagrunt are Polars dataframes. The good news, however, is when you use Datagrunt, you can choose to use Polars or Pandas based on your preference. To convert a Polars dataframe to a Pandas dataframe, simply call to_pandas() as documented here . That said, we'd encourage you to give Polars a try because we think you'll be pleasantly surprised by its performance and simplicity compared to Pandas. Pandas is a wonderful library and we owe a debt of gratitude to Pandas and its creators. However, we feel that in today's world of data that's larger and more complex, dataframe libraries like Polars are more suited to data processing at scale.","title":"Why Polars?"},{"location":"#ideal-use-cases-for-datagrunt","text":"Datagrunt is ideal in the following scenarios: exploratory data analysis (EDA) data engineering pipelines converting small and large CSV files into different file formats with incredible speed","title":"Ideal Use Cases For Datagrunt"},{"location":"#installation","text":"Get started with Datagrunt in seconds using pip: pip install datagrunt","title":"Installation"},{"location":"#getting-started","text":"from Datagrunt.csvfiles import CSVFile # Load your CSV file csv_file = CSVFile('myfile.csv') # Access file information print(f\"File Size: {csv_file.size_in_mb} MB\") print(f\"CSV Attributes: {csv_file.attributes()}\") # Override inferred delimiter (if needed) csv_file.delimiter = '|'","title":"Getting Started"},{"location":"#duckdb-integration-for-performant-sql-queries","text":"from datagrunt.csvfiles import CSVFile csv_file = CSVFile('myfile.csv') # Construct your SQL query query = f\"\"\" SELECT * FROM {csv_file.duckdb_instance.database_table_name} LIMIT 10 \"\"\" # Execute the query and get results as a Polars DataFrame df = csv_file.select_from_table(query) print(df.head())","title":"DuckDB Integration for Performant SQL Queries"},{"location":"#use-datagrunt-with-the-duckdb-python-api","text":"from datagrunt import CSVFile import duckdb csv_file = 'myfile.csv' dg = CSVFile(csv_file) # Datagrunt generates all of the file attributes for you automatically. duckdb.read_csv(dg.filepath, delimiter=dg.delimiter) # use the attributes generated by Datagrunt to interface directly with the DuckDB Python API.","title":"Use Datagrunt With the DuckDB Python API"},{"location":"#contributing","text":"We welcome contributions from the community! If you'd like to contribute to Datagrunt, please follow these steps: 1. Fork the repository. 2. Create a new branch for your feature or bug fix. 3. Write tests to cover your changes. 4. Ensure your code passes all tests and follows the project's style guidelines. 5. Submit a pull request for review.","title":"Contributing"},{"location":"#support","text":"","title":"Support"},{"location":"#roadmap","text":"","title":"Roadmap"},{"location":"#license","text":"This project is licensed under the MIT License","title":"License"},{"location":"#acknowledgements","text":"A HUGE thank you to the open-source community and the creators of DuckDB and Polars for their fantastic libraries that power Datagrunt.","title":"Acknowledgements"},{"location":"api/","text":"API Documentation Project Hierarchy Here is the current application directory structure: \u251c\u2500\u2500 datagrunt \u2502 \u251c\u2500\u2500 csvfiles.py \u2502 \u2514\u2500\u2500 core \u2502 \u251c\u2500\u2500 filehelpers.py \u2502 \u2514\u2500\u2500 databases.py | \u2514\u2500\u2500 logger.py There are two base modules in the core directory: databases.py and filehelpers.py . These two modules provide the core functionality for datagrunt. The databases.py module is designed to support interfacing with databases, including DuckDB . Currently only DuckDB is supported because it's the engine that powers local processing for CSV files. There are plans to support other database platforms in the future, starting with reads, and later adding writes. In the root of the datagrunt directory, csvfiles.py utilize those core modules to power all functionality. Class List \u251c\u2500\u2500 databases.py \u2502 \u2514\u2500\u2500 DuckDBDatabase \u251c\u2500\u2500 filehelpers.py \u2502 \u251c\u2500\u2500 FileBase \u2502 \u251c\u2500\u2500 FileProperties | \u2514\u2500\u2500 CSVParser \u251c\u2500\u2500 logger.py \u2502 \u251c\u2500\u2500 show_warning() \u2502 \u251c\u2500\u2500 show_large_file_warning() \u251c\u2500\u2500 csvfiles.py \u2502 \u2514\u2500\u2500 CSVFile Class Inheritance Order (Single Inheritance) FileBase \u251c\u2500\u2500 FileEvaluator \u2502 \u2514\u2500\u2500CSVParser \u2514\u2500\u2500 CSVFile (DuckDBDatabase is instantiated here, not inherited) Classes FileBase The FileBase class defines what a file is and builds core attributes for a file. Here is a list of attributes in the FileBase class: filepath : str Path to the file. filename : str Filename of the file. Uses the Path module to extract the filename. extension : str Extension of the file. Uses the Path module to extract the extension. extension_string : str String representation of the extension. This removes the . from the extension. size_in_bytes : int Size of the file in bytes. size_in_kb : int Size of the file in kilobytes. size_in_mb : int Size of the file in megabytes. size_in_gb : int Size of the file in gigabytes. size_in_tb : int Size of the file in terabytes. FileEvaluator The FileEvalutor class extends the FileBase class and adds attributes to better understand the nature of the file object. File Type Checks: is_structured : Returns True if the file extension suggests a structured format (CSV, Excel, Parquet, Avro), False otherwise. is_semi_structured : Returns True if the file extension suggests a semi-structured format (JSON), False otherwise. is_unstructured : Returns True if the file extension is not recognized as structured or semi-structured, False otherwise. is_standard : Returns True if the file extension is considered a standard format (CSV, TSV, TXT, JSON), False otherwise. is_proprietary : Returns True if the file extension is associated with a proprietary format (Excel formats), False otherwise. is_csv : Returns True if the file has a CSV-related extension (csv, tsv, txt), False otherwise. is_excel : Returns True if the file has an Excel-related extension (xlsx, xlsm, xlsb, xltx, xltm, xls, xlt), False otherwise. is_apache : Returns True if the file has an Apache-related extension (parquet, avro), False otherwise. File Content/Size Checks: is_empty : Returns True if the file size is 0 bytes, indicating an empty file, False otherwise. is_large : Returns True if the file size is 1 GB or larger, False otherwise. DuckDBDatabase Initialization and Setup __init__(self, filepath): Initializes the DuckDBDatabase object. Takes the filepath of the data file as input. Sets up the database filename (.db extension) and table name based on the input filepath. _set_database_filename(self): Private method (intended for internal use). Generates the filename for the DuckDB database file (e.g., \"data.csv\" becomes \"data.db\"). _set_database_table_name(self): Private method. Generates the table name within the DuckDB database based on the input filename (e.g., \"data.csv\" becomes table \"data\"). set_database_connection(self, threads=DEFAULT_THREAD_COUNT): Property that establishes a connection to the DuckDB database. Takes an optional threads argument (defaults to 16) to configure DuckDB's threading. Returns a DuckDB connection object. SQL Statement Generation select_from_table_statement(self): Generates a simple SQL SELECT statement to retrieve all data from the database table. export_to_csv_statement(self): Generates a SQL COPY statement to export data from the table to a CSV file (\"output.csv\"). export_to_json_array_statement(self): Generates a SQL COPY statement to export data to a JSON file (\"output.json\") with the data enclosed in a JSON array. export_to_json_new_line_delimited_statement(self): Generates a SQL COPY statement to export data to a newline-delimited JSON file (\"output.jsonl\"). export_to_parquet_statement(self): Generates a SQL COPY statement to export data to a Parquet file (\"output.parquet\"). export_to_excel_statement(self): Generates SQL statements to install and load spatial extensions in DuckDB and then export data to an Excel file (\"output.xlsx\"). Data Processing and Export: Return Dataframes or Write Data to a File write_to_file(self, sql_import_statement, sql_export_statement): Executes SQL statements to import data into the DuckDB database and then export it to a file. Takes two SQL statements as input: sql_import_statement and sql_export_statement . The sql_import_statement executes a SQL statement to import data into the DuckDB database. The sql_export_statement executes a SQL statement to export data from the DuckDB database to a file. to_dataframe(self, sql_import_statement): Imports data into the DuckDB database using the provided sql_import_statement . Executes a SELECT statement to retrieve all data from the table. Returns the data as a Polars DataFrame. CSVParser The CSVParser class focuses on parsing CSV files, primarily determining the delimiter used in the file. Here's a breakdown of its methods: Initialization and Setup __init__(self, filepath): Initializes the CSVParser object. Inherits from FileEvaluator to get file properties. Reads the first line of the file and stores it in self.first_row. Infers and stores the CSV delimiter in self.delimiter. Delimiter Inference: _get_first_line_from_file(self): Private method (intended for internal use). Reads and returns the first line of the CSV file, stripping any leading/trailing whitespace. get_most_common_non_alpha_numeric_character_from_string(self): Analyzes the self.first_row to find the most frequent non-alphanumeric character. This is a key step in inferring the delimiter. Returns a list of tuples, where each tuple contains a character and its frequency, sorted by frequency in descending order. infer_csv_file_delimiter(self): Uses the results from get_most_common_non_alpha_numeric_character_from_string() to determine the most likely delimiter. Handles special cases: If the file is empty, it defaults to a comma (,). If no clear delimiter candidate is found, it assumes a space () as the delimiter. Returns the inferred delimiter character. Key Points The CSVParser class is designed to work with various delimiters, not just commas. It uses a heuristic approach to infer the delimiter. This class is intended to be used as a base for other classes that need to work with CSV files, such as the CSVFile class. CSVFile The CSVFile class provides a range of methods for interacting with CSV files, including reading, parsing, converting, and writing data in various formats. Initialization and Setup __init__(self, filepath): Initializes the CSVFile object. Inherits from CSVParser to get delimiter inference functionality. Creates a DuckDBDatabase instance for data processing. Raises a ValueError if the file extension is not a valid CSV extension. Data Retrieval and Transformation _csv_import_table_statement(self): Private method. Generates the SQL statement to import the CSV data into a DuckDB table. Uses all_varchar=True to preserve data integrity by importing all values as strings initially. select_from_table(self, sql_statement): Executes a user-provided SQL statement on the data loaded into the DuckDB table. Allows for custom data transformations using SQL. Returns the result as a Polars DataFrame. File Information and Metadata get_row_count_with_header(self): Returns the total number of rows in the CSV file, including the header row. get_row_count_without_header(self): Returns the number of data rows in the CSV file (excluding the header). get_attributes(self): Generates a dictionary of CSV file attributes: delimiter, quotechar, escapechar, doublequote, newline_delimiter, skipinitialspace, quoting (using CSV dialect sniffing) row_count_with_header, row_count_without_header columns (a dictionary mapping column names to data types, currently all set to 'VARCHAR') and column_count. get_columns(self): Returns the columns dictionary from get_attributes(). get_columns_string(self): Returns the first row of the CSV file as a string, representing the column headers. get_columns_byte_string(self): Returns the first row (column headers) as a byte string. get_column_count(self): Returns the number of columns in the CSV file. get_quotechar(self): Returns the quote character used in the CSV file. get_escapechar(self): Returns the escape character used in the CSV file. get_newline_delimiter(self): Returns the newline delimiter used in the CSV file. Data Conversion and Export to_dicts(self): Reads the CSV data and returns it as a list of dictionaries, where each dictionary represents a row. to_dataframe(self): Reads the CSV data into a Polars DataFrame using DuckDB for efficient processing. to_json(self): Converts the CSV data to a JSON string (using Polars DataFrame's JSON conversion). Prints a warning if the file is large to indicate potential memory issues. to_json_newline_delimited(self): Converts the CSV data to newline-delimited JSON (JSON Lines format). write_avro(self): Writes the data to an Avro file using Polars DataFrame's Avro writing capabilities. write_csv(self): Writes the data to a CSV file using DuckDB's COPY statement. write_json(self): Writes the data to a JSON file using DuckDB's COPY statement (output as a JSON array). write_json_newline_delimited(self): Writes the data to a newline-delimited JSON file using DuckDB. write_parquet(self): Writes the data to a Parquet file using DuckDB. write_excel(self): Writes the data to an Excel file using DuckDB and its spatial extensions. Key Points The CSVFile class leverages DuckDB for efficient data processing and format conversions. It provides a variety of methods for accessing file metadata, converting data formats, and writing data to different file types. The use of Polars DataFrames offers flexibility and performance for data manipulation.","title":"API"},{"location":"api/#api-documentation","text":"","title":"API Documentation"},{"location":"api/#project-hierarchy","text":"Here is the current application directory structure: \u251c\u2500\u2500 datagrunt \u2502 \u251c\u2500\u2500 csvfiles.py \u2502 \u2514\u2500\u2500 core \u2502 \u251c\u2500\u2500 filehelpers.py \u2502 \u2514\u2500\u2500 databases.py | \u2514\u2500\u2500 logger.py There are two base modules in the core directory: databases.py and filehelpers.py . These two modules provide the core functionality for datagrunt. The databases.py module is designed to support interfacing with databases, including DuckDB . Currently only DuckDB is supported because it's the engine that powers local processing for CSV files. There are plans to support other database platforms in the future, starting with reads, and later adding writes. In the root of the datagrunt directory, csvfiles.py utilize those core modules to power all functionality.","title":"Project Hierarchy"},{"location":"api/#class-list","text":"\u251c\u2500\u2500 databases.py \u2502 \u2514\u2500\u2500 DuckDBDatabase \u251c\u2500\u2500 filehelpers.py \u2502 \u251c\u2500\u2500 FileBase \u2502 \u251c\u2500\u2500 FileProperties | \u2514\u2500\u2500 CSVParser \u251c\u2500\u2500 logger.py \u2502 \u251c\u2500\u2500 show_warning() \u2502 \u251c\u2500\u2500 show_large_file_warning() \u251c\u2500\u2500 csvfiles.py \u2502 \u2514\u2500\u2500 CSVFile","title":"Class List"},{"location":"api/#class-inheritance-order-single-inheritance","text":"FileBase \u251c\u2500\u2500 FileEvaluator \u2502 \u2514\u2500\u2500CSVParser \u2514\u2500\u2500 CSVFile (DuckDBDatabase is instantiated here, not inherited)","title":"Class Inheritance Order (Single Inheritance)"},{"location":"api/#classes","text":"","title":"Classes"},{"location":"api/#filebase","text":"The FileBase class defines what a file is and builds core attributes for a file. Here is a list of attributes in the FileBase class: filepath : str Path to the file. filename : str Filename of the file. Uses the Path module to extract the filename. extension : str Extension of the file. Uses the Path module to extract the extension. extension_string : str String representation of the extension. This removes the . from the extension. size_in_bytes : int Size of the file in bytes. size_in_kb : int Size of the file in kilobytes. size_in_mb : int Size of the file in megabytes. size_in_gb : int Size of the file in gigabytes. size_in_tb : int Size of the file in terabytes.","title":"FileBase"},{"location":"api/#fileevaluator","text":"The FileEvalutor class extends the FileBase class and adds attributes to better understand the nature of the file object. File Type Checks: is_structured : Returns True if the file extension suggests a structured format (CSV, Excel, Parquet, Avro), False otherwise. is_semi_structured : Returns True if the file extension suggests a semi-structured format (JSON), False otherwise. is_unstructured : Returns True if the file extension is not recognized as structured or semi-structured, False otherwise. is_standard : Returns True if the file extension is considered a standard format (CSV, TSV, TXT, JSON), False otherwise. is_proprietary : Returns True if the file extension is associated with a proprietary format (Excel formats), False otherwise. is_csv : Returns True if the file has a CSV-related extension (csv, tsv, txt), False otherwise. is_excel : Returns True if the file has an Excel-related extension (xlsx, xlsm, xlsb, xltx, xltm, xls, xlt), False otherwise. is_apache : Returns True if the file has an Apache-related extension (parquet, avro), False otherwise. File Content/Size Checks: is_empty : Returns True if the file size is 0 bytes, indicating an empty file, False otherwise. is_large : Returns True if the file size is 1 GB or larger, False otherwise.","title":"FileEvaluator"},{"location":"api/#duckdbdatabase","text":"Initialization and Setup __init__(self, filepath): Initializes the DuckDBDatabase object. Takes the filepath of the data file as input. Sets up the database filename (.db extension) and table name based on the input filepath. _set_database_filename(self): Private method (intended for internal use). Generates the filename for the DuckDB database file (e.g., \"data.csv\" becomes \"data.db\"). _set_database_table_name(self): Private method. Generates the table name within the DuckDB database based on the input filename (e.g., \"data.csv\" becomes table \"data\"). set_database_connection(self, threads=DEFAULT_THREAD_COUNT): Property that establishes a connection to the DuckDB database. Takes an optional threads argument (defaults to 16) to configure DuckDB's threading. Returns a DuckDB connection object. SQL Statement Generation select_from_table_statement(self): Generates a simple SQL SELECT statement to retrieve all data from the database table. export_to_csv_statement(self): Generates a SQL COPY statement to export data from the table to a CSV file (\"output.csv\"). export_to_json_array_statement(self): Generates a SQL COPY statement to export data to a JSON file (\"output.json\") with the data enclosed in a JSON array. export_to_json_new_line_delimited_statement(self): Generates a SQL COPY statement to export data to a newline-delimited JSON file (\"output.jsonl\"). export_to_parquet_statement(self): Generates a SQL COPY statement to export data to a Parquet file (\"output.parquet\"). export_to_excel_statement(self): Generates SQL statements to install and load spatial extensions in DuckDB and then export data to an Excel file (\"output.xlsx\"). Data Processing and Export: Return Dataframes or Write Data to a File write_to_file(self, sql_import_statement, sql_export_statement): Executes SQL statements to import data into the DuckDB database and then export it to a file. Takes two SQL statements as input: sql_import_statement and sql_export_statement . The sql_import_statement executes a SQL statement to import data into the DuckDB database. The sql_export_statement executes a SQL statement to export data from the DuckDB database to a file. to_dataframe(self, sql_import_statement): Imports data into the DuckDB database using the provided sql_import_statement . Executes a SELECT statement to retrieve all data from the table. Returns the data as a Polars DataFrame.","title":"DuckDBDatabase"},{"location":"api/#csvparser","text":"The CSVParser class focuses on parsing CSV files, primarily determining the delimiter used in the file. Here's a breakdown of its methods: Initialization and Setup __init__(self, filepath): Initializes the CSVParser object. Inherits from FileEvaluator to get file properties. Reads the first line of the file and stores it in self.first_row. Infers and stores the CSV delimiter in self.delimiter. Delimiter Inference: _get_first_line_from_file(self): Private method (intended for internal use). Reads and returns the first line of the CSV file, stripping any leading/trailing whitespace. get_most_common_non_alpha_numeric_character_from_string(self): Analyzes the self.first_row to find the most frequent non-alphanumeric character. This is a key step in inferring the delimiter. Returns a list of tuples, where each tuple contains a character and its frequency, sorted by frequency in descending order. infer_csv_file_delimiter(self): Uses the results from get_most_common_non_alpha_numeric_character_from_string() to determine the most likely delimiter. Handles special cases: If the file is empty, it defaults to a comma (,). If no clear delimiter candidate is found, it assumes a space () as the delimiter. Returns the inferred delimiter character. Key Points The CSVParser class is designed to work with various delimiters, not just commas. It uses a heuristic approach to infer the delimiter. This class is intended to be used as a base for other classes that need to work with CSV files, such as the CSVFile class.","title":"CSVParser"},{"location":"api/#csvfile","text":"The CSVFile class provides a range of methods for interacting with CSV files, including reading, parsing, converting, and writing data in various formats. Initialization and Setup __init__(self, filepath): Initializes the CSVFile object. Inherits from CSVParser to get delimiter inference functionality. Creates a DuckDBDatabase instance for data processing. Raises a ValueError if the file extension is not a valid CSV extension. Data Retrieval and Transformation _csv_import_table_statement(self): Private method. Generates the SQL statement to import the CSV data into a DuckDB table. Uses all_varchar=True to preserve data integrity by importing all values as strings initially. select_from_table(self, sql_statement): Executes a user-provided SQL statement on the data loaded into the DuckDB table. Allows for custom data transformations using SQL. Returns the result as a Polars DataFrame. File Information and Metadata get_row_count_with_header(self): Returns the total number of rows in the CSV file, including the header row. get_row_count_without_header(self): Returns the number of data rows in the CSV file (excluding the header). get_attributes(self): Generates a dictionary of CSV file attributes: delimiter, quotechar, escapechar, doublequote, newline_delimiter, skipinitialspace, quoting (using CSV dialect sniffing) row_count_with_header, row_count_without_header columns (a dictionary mapping column names to data types, currently all set to 'VARCHAR') and column_count. get_columns(self): Returns the columns dictionary from get_attributes(). get_columns_string(self): Returns the first row of the CSV file as a string, representing the column headers. get_columns_byte_string(self): Returns the first row (column headers) as a byte string. get_column_count(self): Returns the number of columns in the CSV file. get_quotechar(self): Returns the quote character used in the CSV file. get_escapechar(self): Returns the escape character used in the CSV file. get_newline_delimiter(self): Returns the newline delimiter used in the CSV file. Data Conversion and Export to_dicts(self): Reads the CSV data and returns it as a list of dictionaries, where each dictionary represents a row. to_dataframe(self): Reads the CSV data into a Polars DataFrame using DuckDB for efficient processing. to_json(self): Converts the CSV data to a JSON string (using Polars DataFrame's JSON conversion). Prints a warning if the file is large to indicate potential memory issues. to_json_newline_delimited(self): Converts the CSV data to newline-delimited JSON (JSON Lines format). write_avro(self): Writes the data to an Avro file using Polars DataFrame's Avro writing capabilities. write_csv(self): Writes the data to a CSV file using DuckDB's COPY statement. write_json(self): Writes the data to a JSON file using DuckDB's COPY statement (output as a JSON array). write_json_newline_delimited(self): Writes the data to a newline-delimited JSON file using DuckDB. write_parquet(self): Writes the data to a Parquet file using DuckDB. write_excel(self): Writes the data to an Excel file using DuckDB and its spatial extensions. Key Points The CSVFile class leverages DuckDB for efficient data processing and format conversions. It provides a variety of methods for accessing file metadata, converting data formats, and writing data to different file types. The use of Polars DataFrames offers flexibility and performance for data manipulation.","title":"CSVFile"}]}