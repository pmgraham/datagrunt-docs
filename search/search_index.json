{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Datagrunt! Datagrunt is a Python library designed to simplify the way you work with CSV files. It provides a streamlined approach to reading, processing, and transforming your data into various formats, making data manipulation efficient and intuitive. Why Datagrunt? Born out of real-world frustration, Datagrunt eliminates the need For repetitive coding when handling CSV files. Whether you're a data analyst, data engineer, or data scientist, Datagrunt empowers you to focus on insights, not tedious data wrangling. Key Features Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for your csv files. Seamless Data Processing: Leverage the robust capabilities of DuckDB and Polars to perform advanced data processing tasks directly on your CSV data. Flexible Transformation: Easily convert your processed CSV data into various formats to suit your needs. Pythonic API: Enjoy a clean and intuitive API that integrates seamlessly into your existing Python workflows. Power Tools Under the Hood Tool Description DuckDB Fast in-process analytical database with a simple Python API Polars Multi-threaded query engine written in Rust, optimized for modern processors Datagrunt's Role Datagrunt is not an extension of DuckDB or Polars, nor a comprehensive data processing solution. Its primary functions are: Accurately inferring CSV delimiters Providing helper methods for common data tasks Facilitating CSV file loading into Polars dataframes Enabling conversion to various output formats Flexibility and Integration While Datagrunt uses Polars dataframes by default, it doesn't limit your options: Convert to Pandas dataframes using the to_pandas method Use Datagrunt's output in other contexts within your script or application Leverage only specific features (e.g., delimiter inference) as needed Datagrunt complements your existing data processing toolkit, offering helpful utilities without restricting your workflow choices. Code Repository You can find the Datagrunt source code on GitHub","title":"Home"},{"location":"#welcome-to-datagrunt","text":"Datagrunt is a Python library designed to simplify the way you work with CSV files. It provides a streamlined approach to reading, processing, and transforming your data into various formats, making data manipulation efficient and intuitive.","title":"Welcome to Datagrunt!"},{"location":"#why-datagrunt","text":"Born out of real-world frustration, Datagrunt eliminates the need For repetitive coding when handling CSV files. Whether you're a data analyst, data engineer, or data scientist, Datagrunt empowers you to focus on insights, not tedious data wrangling.","title":"Why Datagrunt?"},{"location":"#key-features","text":"Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for your csv files. Seamless Data Processing: Leverage the robust capabilities of DuckDB and Polars to perform advanced data processing tasks directly on your CSV data. Flexible Transformation: Easily convert your processed CSV data into various formats to suit your needs. Pythonic API: Enjoy a clean and intuitive API that integrates seamlessly into your existing Python workflows.","title":"Key Features"},{"location":"#power-tools-under-the-hood","text":"Tool Description DuckDB Fast in-process analytical database with a simple Python API Polars Multi-threaded query engine written in Rust, optimized for modern processors","title":"Power Tools Under the Hood"},{"location":"#datagrunts-role","text":"Datagrunt is not an extension of DuckDB or Polars, nor a comprehensive data processing solution. Its primary functions are: Accurately inferring CSV delimiters Providing helper methods for common data tasks Facilitating CSV file loading into Polars dataframes Enabling conversion to various output formats","title":"Datagrunt's Role"},{"location":"#flexibility-and-integration","text":"While Datagrunt uses Polars dataframes by default, it doesn't limit your options: Convert to Pandas dataframes using the to_pandas method Use Datagrunt's output in other contexts within your script or application Leverage only specific features (e.g., delimiter inference) as needed Datagrunt complements your existing data processing toolkit, offering helpful utilities without restricting your workflow choices.","title":"Flexibility and Integration"},{"location":"#code-repository","text":"You can find the Datagrunt source code on GitHub","title":"Code Repository"},{"location":"api/","text":"API Reference Class Hierarchy Here is the hierarchy of classes for Datagrunt: FileProperties : This class serves as the foundation for defining core file properties for any file type. CSVProperties : Inherits from FileProperties. Adds properties specifically for CSV files. An error is raised if any file without the .csv file extension. CSVReader : Inherits from CSVProperties and adds helper methods for reading and querying CSV files. CSVWriter : Inherits from CSVProperties and adds helper methods for converting CSV files to various other supported file types. FileProperties The base class for file objects. __init__(filepath) : Initialize the FileProperties with a CSV file path. Properties filepath : Path to the file. filename : Name of the file without the full path. extension : File extension (with dot). extension_string : File extension (without dot). size_in_bytes : File size in bytes. size_in_kb : File size in kilobytes. size_in_mb : File size in megabytes. size_in_gb : File size in gigabytes. size_in_tb : File size in terabytes. is_structured : Boolean indicating if the file is structured data. is_semi_structured : Boolean indicating if the file is semi-structured data. is_unstructured : Boolean indicating if the file is unstructured data. is_standard : Boolean indicating if the file has a standard (open and non proprietary) format. is_proprietary : Boolean indicating if the file has a proprietary format. is_csv : Boolean indicating if the file is a CSV. is_excel : Boolean indicating if the file is an Excel file. is_apache : Boolean indicating if the file is an Apache formatted file (Avro or Parquet). is_empty : Boolean indicating if the file is empty. is_large : Boolean indicating if the file is larger than or equal to 1 GB. is_tabular : Boolean indicating if the file is tabular. CSVProperties A subclass of FileProperties, specifically for CSV files. Initialized with a CSV file path. Properties (in addition to FileProperties) delimiter : The delimiter used in the CSV file. Delimiter is automatically inferred and may be optionally re-assigned by the user. row_count_with_header : Number of rows including the first line. row_count_without_header : Number of rows excluding the first line. columns : List of column names. columns_string : Comma-separated string of column names. columns_byte_string : Byte string of comma-separated column names. column_count : Number of columns. quotechar : Quote character used in the CSV. escapechar : Escape character used in the CSV. newline_delimiter : Newline delimiter used in the CSV. CSVReader A class for reading and querying CSV files. Properties (User provided) filepath : Path to the CSV file. Required and provided the user. engine : The engine being used ('polars' or 'duckdb'). Optional. Default is 'polars'. Updated by the user as needed. Properties (Built into the class) db_table : The name of the table in DuckDB where the CSV data is imported for processing. Built into the class with no intervention from the user required. Methods __init__(filepath: str, engine: str = 'polars') -> None Initialize the CSVReader with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for reading ('polars' or 'duckdb'). Defaults to 'polars'. Returns: None get_sample() -> Union[pl.DataFrame, duckdb.DuckDBPyRelation] Return a sample of the CSV file. Args: None Returns: A sample of the data (Polars DataFrame or DuckDB Relation, depending on the engine). to_dataframe() -> pl.DataFrame Convert the CSV to a Polars dataframe. Args: None Returns: A Polars DataFrame containing the CSV data. to_arrow_table() -> pa.Table Convert the CSV to a PyArrow table. Args: None Returns: A PyArrow Table containing the CSV data. to_dicts() -> List[Dict[str, Any]] Convert the CSV to a list of dictionaries. Args: None Returns: A list of dictionaries, where each dictionary represents a row in the CSV. query_data(sql_query: str) -> duckdb.DuckDBPyRelation Query the CSV file using SQL (via DuckDB). Args: sql_query (str): SQL query to execute against the CSV data. Returns: A DuckDB Relation containing the query results. CSVWriter A class for converting CSV files to various other supported file types. Properties (User provided) filepath : Path to the CSV file. Required and provided the user. engine : The engine being used ('polars' or 'duckdb'). Optional. Default is 'polars'. Updated by the user as needed. Properties (Built into the class) db_table : The name of the table in DuckDB where the CSV data is imported for processing. Built into the class with no intervention from the user required. Methods __init__(filepath: str, engine: str = 'duckdb') -> None Initialize the CSVWriter with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for writing ('polars' or 'duckdb'). Defaults to 'duckdb'. Returns: None write_csv(out_filename: Optional[str] = None) -> str Write the CSV data to a new CSV file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written CSV file. write_excel(out_filename: Optional[str] = None) -> str Write the CSV data to an Excel file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Excel file. write_json(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON file. write_json_newline_delimited(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON newline delimited file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON newline delimited file. write_parquet(out_filename: Optional[str] = None) -> str Write the CSV data to a Parquet file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Parquet file.","title":"API"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#class-hierarchy","text":"Here is the hierarchy of classes for Datagrunt: FileProperties : This class serves as the foundation for defining core file properties for any file type. CSVProperties : Inherits from FileProperties. Adds properties specifically for CSV files. An error is raised if any file without the .csv file extension. CSVReader : Inherits from CSVProperties and adds helper methods for reading and querying CSV files. CSVWriter : Inherits from CSVProperties and adds helper methods for converting CSV files to various other supported file types.","title":"Class Hierarchy"},{"location":"api/#fileproperties","text":"The base class for file objects. __init__(filepath) : Initialize the FileProperties with a CSV file path.","title":"FileProperties"},{"location":"api/#properties","text":"filepath : Path to the file. filename : Name of the file without the full path. extension : File extension (with dot). extension_string : File extension (without dot). size_in_bytes : File size in bytes. size_in_kb : File size in kilobytes. size_in_mb : File size in megabytes. size_in_gb : File size in gigabytes. size_in_tb : File size in terabytes. is_structured : Boolean indicating if the file is structured data. is_semi_structured : Boolean indicating if the file is semi-structured data. is_unstructured : Boolean indicating if the file is unstructured data. is_standard : Boolean indicating if the file has a standard (open and non proprietary) format. is_proprietary : Boolean indicating if the file has a proprietary format. is_csv : Boolean indicating if the file is a CSV. is_excel : Boolean indicating if the file is an Excel file. is_apache : Boolean indicating if the file is an Apache formatted file (Avro or Parquet). is_empty : Boolean indicating if the file is empty. is_large : Boolean indicating if the file is larger than or equal to 1 GB. is_tabular : Boolean indicating if the file is tabular.","title":"Properties"},{"location":"api/#csvproperties","text":"A subclass of FileProperties, specifically for CSV files. Initialized with a CSV file path.","title":"CSVProperties"},{"location":"api/#properties-in-addition-to-fileproperties","text":"delimiter : The delimiter used in the CSV file. Delimiter is automatically inferred and may be optionally re-assigned by the user. row_count_with_header : Number of rows including the first line. row_count_without_header : Number of rows excluding the first line. columns : List of column names. columns_string : Comma-separated string of column names. columns_byte_string : Byte string of comma-separated column names. column_count : Number of columns. quotechar : Quote character used in the CSV. escapechar : Escape character used in the CSV. newline_delimiter : Newline delimiter used in the CSV.","title":"Properties (in addition to FileProperties)"},{"location":"api/#csvreader","text":"A class for reading and querying CSV files.","title":"CSVReader"},{"location":"api/#properties-user-provided","text":"filepath : Path to the CSV file. Required and provided the user. engine : The engine being used ('polars' or 'duckdb'). Optional. Default is 'polars'. Updated by the user as needed.","title":"Properties (User provided)"},{"location":"api/#properties-built-into-the-class","text":"db_table : The name of the table in DuckDB where the CSV data is imported for processing. Built into the class with no intervention from the user required.","title":"Properties (Built into the class)"},{"location":"api/#methods","text":"__init__(filepath: str, engine: str = 'polars') -> None Initialize the CSVReader with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for reading ('polars' or 'duckdb'). Defaults to 'polars'. Returns: None get_sample() -> Union[pl.DataFrame, duckdb.DuckDBPyRelation] Return a sample of the CSV file. Args: None Returns: A sample of the data (Polars DataFrame or DuckDB Relation, depending on the engine). to_dataframe() -> pl.DataFrame Convert the CSV to a Polars dataframe. Args: None Returns: A Polars DataFrame containing the CSV data. to_arrow_table() -> pa.Table Convert the CSV to a PyArrow table. Args: None Returns: A PyArrow Table containing the CSV data. to_dicts() -> List[Dict[str, Any]] Convert the CSV to a list of dictionaries. Args: None Returns: A list of dictionaries, where each dictionary represents a row in the CSV. query_data(sql_query: str) -> duckdb.DuckDBPyRelation Query the CSV file using SQL (via DuckDB). Args: sql_query (str): SQL query to execute against the CSV data. Returns: A DuckDB Relation containing the query results.","title":"Methods"},{"location":"api/#csvwriter","text":"A class for converting CSV files to various other supported file types.","title":"CSVWriter"},{"location":"api/#properties-user-provided_1","text":"filepath : Path to the CSV file. Required and provided the user. engine : The engine being used ('polars' or 'duckdb'). Optional. Default is 'polars'. Updated by the user as needed.","title":"Properties (User provided)"},{"location":"api/#properties-built-into-the-class_1","text":"db_table : The name of the table in DuckDB where the CSV data is imported for processing. Built into the class with no intervention from the user required.","title":"Properties (Built into the class)"},{"location":"api/#methods_1","text":"__init__(filepath: str, engine: str = 'duckdb') -> None Initialize the CSVWriter with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for writing ('polars' or 'duckdb'). Defaults to 'duckdb'. Returns: None write_csv(out_filename: Optional[str] = None) -> str Write the CSV data to a new CSV file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written CSV file. write_excel(out_filename: Optional[str] = None) -> str Write the CSV data to an Excel file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Excel file. write_json(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON file. write_json_newline_delimited(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON newline delimited file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON newline delimited file. write_parquet(out_filename: Optional[str] = None) -> str Write the CSV data to a Parquet file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Parquet file.","title":"Methods"},{"location":"examples/","text":"Usage Examples Datagrunt offers a variety of functionalities. Here are some common use cases to help you get started. Setting The Processing Engine Datagrunt gives you two options for CSV file processing: DuckDB and Polars. You set the engine as follows: from datagrunt import CSVReader csv_file = 'myfile.csv' dg = CSVReader(csv_file) # by default engine is set to 'polars'. Alternatively, you may set the processing engine to DuckDB by passing the 'duckdb' value for the engine param as follows: from datagrunt import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) The interface is the exact same for both engines. If you write your code using Polars, but want to change to DuckDB later, simply update the engine param when you instantiate the CSVReader class and leave the rest of your code untouched. The engine functionality works exactly the same way for the CSVWriter class as well. Here is an example using the Polars engine: from datagrunt import CSVWriter csv_file = 'myfile.csv' dg = CSVWriter(csv_file) dg.write_json() Here is an example using the DuckDB engine: from datagrunt import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVWriter(csv_file, engine=engine) dg.write_json() Working with CSV Properties Get CSV Delimiter from datagrunt import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) delimiter = csv_props.delimiter # Optional: Use delimiter.encode() to return a bytestring Manually Set CSV Delimiter from datagrunt import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) csv_props.delimiter = ',' # Override the inferred delimiter Sampling CSV Data from datagrunt import CSVReader csv_file = 'myfile.csv' # Sample using DuckDB (prints to screen automatically) CSVReader(csv_file, engine='duckdb').get_sample() # Sample using Polars (prints to screen automatically) CSVReader(csv_file).get_sample() Reading CSV Files Using DuckDB from datagrunt import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' duckdb_reader = CSVReader(csv_file, engine=engine) df_duckdb = duckdb_reader.to_dataframe() Using Polars from datagrunt import CSVReader csv_file = 'myfile.csv' polars_reader = CSVReader(csv_file) df_polars = polars_reader.to_dataframe() Writing CSV to Other Formats JSON and JSON Lines with DuckDB from datagrunt import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' duckdb_writer = CSVWriter(csv_file, engine=engine) # Write to JSON duckdb_writer.write_json() # Optional: Specify output filename # Write to JSON Lines duckdb_writer.write_json_newline_delimited() JSON and JSON Lines with Polars from datagrunt import CSVWriter csv_file = 'myfile.csv' polars_writer = CSVWriter(csv_file) # Write to JSON polars_writer.write_json() # Optional: Specify output filename # Write to JSON Lines polars_writer.write_json_newline_delimited() # Use .jsonl extension if specifying filename Integration with Pandas from datagrunt import CSVReader import pandas as pd csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) df_polars = csv_props.to_dataframe() # returns Polars dataframe automatically # convert Polars dataframe to Pandas df_pandas_from_polars = df_polars.to_pandas() # Use Datagrunt to get the correct delimiter for Pandas df_pandas = pd.read_csv(csv_file, delimiter=csv_props.delimiter) Using SQL To Query CSV Files It's recommened to use the duckdb engine when querying CSV files using SQL. These query examples will work with the polars engine exactly the same way as presented, but Datagrunt converts all data to strings when using duckdb and it better preserves the integrity of the data due to not coercing into inferred datatypes. You'll see below that you can use SQL and you can optionally cast columns to different data types as needed. In the below example there are postal codes in the data. Usually, postal codes are read as integers by most processing engines. However, many postal codes begin with a leading 0, and in that case if postal codes are read as an integer, the leading 0 will be dropped erroneously. This can be fixed by using the zfill function in Python, but we'd prefer to present the data in its original form as a string as opposed to having the data altered due to data type conversion. Zipcodes are only one of many examples where data integrity may be lost due to type conversions. Therefore, we do our best to preserve the data in its original state, even if it means we have to type case downstream. from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() # only a sample output below is represented. VIN (1-10) County City State DOL Vehicle ID Vehicle Location Electric Utility 2020 Census Tract 5YJSA1E28K Snohomish Mukilteo WA 236424583 POINT (-122.29943 47.912654) PUGET SOUND ENERGY INC 53061042001 1C4JJXP68P Yakima Yakima WA 249905295 POINT (-120.468875 46.6046178) PACIFICORP 53077001601 WBY8P6C05L Kitsap Kingston WA 260917289 POINT (-122.5178351 47.7981436) PUGET SOUND ENERGY INC 53035090102 JTDKARFP1J Kitsap Port Orchard WA 186410087 POINT (-122.6530052 47.4739066) PUGET SOUND ENERGY INC 53035092802 5UXTA6C09N Snohomish Everett WA 186076915 POINT (-122.2032349 47.8956271) PUGET SOUND ENERGY INC 53061041605 JTMAB3FVXR Snohomish Snohomish WA 262809249 POINT (-122.0483457 47.9435765) PUGET SOUND ENERGY INC 53061052402 7FCTGAAA7P Pierce Orting WA 252195450 POINT (-122.1977914 47.0948565) PUGET SOUND ENERGY INC||CITY O... 53053070100 1V2GNPE87P Spokane Spokane WA 227314790 POINT (-117.428902 47.658268) MO Write a SQL Query from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. # the db_table object is an instance attribute provided by the CSVReader class. It's a temporary object created by DuckDB for local data processing. query = f\"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM {dg.db_table} ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 ... ... Vista 1 Tempe 1 Green Bay 1 Waverly 1 Creston 1 764 rows (20 shown) 2 columns Query a Polars Dataframe Note: in the code example below that we are not interpolating the dataframe variable. We are referencing it by embedding it in the string. from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() # notice that the dataframe object df is not interpolated. df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(df_query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 Kirkland 5883 Renton 5835 Sammamish 5795 Olympia 4830 Tacoma 4204 ... ... ... ... ... ... Lamont 1 Dickinson 1 Saratoga Springs 1 Gunpowder 1 Holden Village 1 Yorktown 1 Ridgecrest 1 Startup 1 Sacramento 1 Washtucna 1 764 rows (20 shown) 2 columns Query Results To Dataframe In the previous example we demonstrated querying a Polars dataframe by using SQL. Rather than simply returning results, you may convert the results to any type of dataframe you wish. from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" df_polars = dg.query_data(df_query).pl() # return a Polars dataframe df_pandas = dg.query_data(df_query).to_df() # return a Pandas dataframe Credits and References The SQL query functionality within Datagrunt is powered by DuckDB . The dataset used in these code examples is sourced from Electric Vehicle Population Data .","title":"Usage Examples"},{"location":"examples/#usage-examples","text":"Datagrunt offers a variety of functionalities. Here are some common use cases to help you get started.","title":"Usage Examples"},{"location":"examples/#setting-the-processing-engine","text":"Datagrunt gives you two options for CSV file processing: DuckDB and Polars. You set the engine as follows: from datagrunt import CSVReader csv_file = 'myfile.csv' dg = CSVReader(csv_file) # by default engine is set to 'polars'. Alternatively, you may set the processing engine to DuckDB by passing the 'duckdb' value for the engine param as follows: from datagrunt import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) The interface is the exact same for both engines. If you write your code using Polars, but want to change to DuckDB later, simply update the engine param when you instantiate the CSVReader class and leave the rest of your code untouched. The engine functionality works exactly the same way for the CSVWriter class as well. Here is an example using the Polars engine: from datagrunt import CSVWriter csv_file = 'myfile.csv' dg = CSVWriter(csv_file) dg.write_json() Here is an example using the DuckDB engine: from datagrunt import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVWriter(csv_file, engine=engine) dg.write_json()","title":"Setting The Processing Engine"},{"location":"examples/#working-with-csv-properties","text":"","title":"Working with CSV Properties"},{"location":"examples/#get-csv-delimiter","text":"from datagrunt import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) delimiter = csv_props.delimiter # Optional: Use delimiter.encode() to return a bytestring","title":"Get CSV Delimiter"},{"location":"examples/#manually-set-csv-delimiter","text":"from datagrunt import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) csv_props.delimiter = ',' # Override the inferred delimiter","title":"Manually Set CSV Delimiter"},{"location":"examples/#sampling-csv-data","text":"from datagrunt import CSVReader csv_file = 'myfile.csv' # Sample using DuckDB (prints to screen automatically) CSVReader(csv_file, engine='duckdb').get_sample() # Sample using Polars (prints to screen automatically) CSVReader(csv_file).get_sample()","title":"Sampling CSV Data"},{"location":"examples/#reading-csv-files","text":"","title":"Reading CSV Files"},{"location":"examples/#using-duckdb","text":"from datagrunt import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' duckdb_reader = CSVReader(csv_file, engine=engine) df_duckdb = duckdb_reader.to_dataframe()","title":"Using DuckDB"},{"location":"examples/#using-polars","text":"from datagrunt import CSVReader csv_file = 'myfile.csv' polars_reader = CSVReader(csv_file) df_polars = polars_reader.to_dataframe()","title":"Using Polars"},{"location":"examples/#writing-csv-to-other-formats","text":"","title":"Writing CSV to Other Formats"},{"location":"examples/#json-and-json-lines-with-duckdb","text":"from datagrunt import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' duckdb_writer = CSVWriter(csv_file, engine=engine) # Write to JSON duckdb_writer.write_json() # Optional: Specify output filename # Write to JSON Lines duckdb_writer.write_json_newline_delimited()","title":"JSON and JSON Lines with DuckDB"},{"location":"examples/#json-and-json-lines-with-polars","text":"from datagrunt import CSVWriter csv_file = 'myfile.csv' polars_writer = CSVWriter(csv_file) # Write to JSON polars_writer.write_json() # Optional: Specify output filename # Write to JSON Lines polars_writer.write_json_newline_delimited() # Use .jsonl extension if specifying filename","title":"JSON and JSON Lines with Polars"},{"location":"examples/#integration-with-pandas","text":"from datagrunt import CSVReader import pandas as pd csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) df_polars = csv_props.to_dataframe() # returns Polars dataframe automatically # convert Polars dataframe to Pandas df_pandas_from_polars = df_polars.to_pandas() # Use Datagrunt to get the correct delimiter for Pandas df_pandas = pd.read_csv(csv_file, delimiter=csv_props.delimiter)","title":"Integration with Pandas"},{"location":"examples/#using-sql-to-query-csv-files","text":"It's recommened to use the duckdb engine when querying CSV files using SQL. These query examples will work with the polars engine exactly the same way as presented, but Datagrunt converts all data to strings when using duckdb and it better preserves the integrity of the data due to not coercing into inferred datatypes. You'll see below that you can use SQL and you can optionally cast columns to different data types as needed. In the below example there are postal codes in the data. Usually, postal codes are read as integers by most processing engines. However, many postal codes begin with a leading 0, and in that case if postal codes are read as an integer, the leading 0 will be dropped erroneously. This can be fixed by using the zfill function in Python, but we'd prefer to present the data in its original form as a string as opposed to having the data altered due to data type conversion. Zipcodes are only one of many examples where data integrity may be lost due to type conversions. Therefore, we do our best to preserve the data in its original state, even if it means we have to type case downstream. from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() # only a sample output below is represented. VIN (1-10) County City State DOL Vehicle ID Vehicle Location Electric Utility 2020 Census Tract 5YJSA1E28K Snohomish Mukilteo WA 236424583 POINT (-122.29943 47.912654) PUGET SOUND ENERGY INC 53061042001 1C4JJXP68P Yakima Yakima WA 249905295 POINT (-120.468875 46.6046178) PACIFICORP 53077001601 WBY8P6C05L Kitsap Kingston WA 260917289 POINT (-122.5178351 47.7981436) PUGET SOUND ENERGY INC 53035090102 JTDKARFP1J Kitsap Port Orchard WA 186410087 POINT (-122.6530052 47.4739066) PUGET SOUND ENERGY INC 53035092802 5UXTA6C09N Snohomish Everett WA 186076915 POINT (-122.2032349 47.8956271) PUGET SOUND ENERGY INC 53061041605 JTMAB3FVXR Snohomish Snohomish WA 262809249 POINT (-122.0483457 47.9435765) PUGET SOUND ENERGY INC 53061052402 7FCTGAAA7P Pierce Orting WA 252195450 POINT (-122.1977914 47.0948565) PUGET SOUND ENERGY INC||CITY O... 53053070100 1V2GNPE87P Spokane Spokane WA 227314790 POINT (-117.428902 47.658268) MO","title":"Using SQL To Query CSV Files"},{"location":"examples/#write-a-sql-query","text":"from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. # the db_table object is an instance attribute provided by the CSVReader class. It's a temporary object created by DuckDB for local data processing. query = f\"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM {dg.db_table} ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 ... ... Vista 1 Tempe 1 Green Bay 1 Waverly 1 Creston 1 764 rows (20 shown) 2 columns","title":"Write a SQL Query"},{"location":"examples/#query-a-polars-dataframe","text":"Note: in the code example below that we are not interpolating the dataframe variable. We are referencing it by embedding it in the string. from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() # notice that the dataframe object df is not interpolated. df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(df_query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 Kirkland 5883 Renton 5835 Sammamish 5795 Olympia 4830 Tacoma 4204 ... ... ... ... ... ... Lamont 1 Dickinson 1 Saratoga Springs 1 Gunpowder 1 Holden Village 1 Yorktown 1 Ridgecrest 1 Startup 1 Sacramento 1 Washtucna 1 764 rows (20 shown) 2 columns","title":"Query a Polars Dataframe"},{"location":"examples/#query-results-to-dataframe","text":"In the previous example we demonstrated querying a Polars dataframe by using SQL. Rather than simply returning results, you may convert the results to any type of dataframe you wish. from datagrunt import CSVReader csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" df_polars = dg.query_data(df_query).pl() # return a Polars dataframe df_pandas = dg.query_data(df_query).to_df() # return a Pandas dataframe","title":"Query Results To Dataframe"},{"location":"examples/#credits-and-references","text":"The SQL query functionality within Datagrunt is powered by DuckDB . The dataset used in these code examples is sourced from Electric Vehicle Population Data .","title":"Credits and References"}]}