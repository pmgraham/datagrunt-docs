{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Datagrunt! Datagrunt is a lightweight CSV processing library that simplifies handling CSV files and converting them into various formats such as Excel, JSON, JSON Newline Delimited, and Parquet. It eliminates repetitive coding tasks when working with CSVs, empowering data analysts, engineers, and scientists to focus on insights rather than tedious data wrangling. Key Features Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for CSV files using a custom algorithm powered by DuckDB. Seamless Data Processing: Leverage DuckDB and Polars to perform data processing tasks directly on CSV data. Flexible Transformation: Easily convert processed CSV data into different formats to meet your needs. Pythonic API: Enjoy a clean and intuitive API that seamlessly integrates into existing Python workflows. About DuckDB and Polars Processing CSV files, especially large ones locally or in resource-constrained data pipelines, can be challenging. Python, while powerful, is not known for its speed in heavy data processing tasks. DuckDB and Polars address this limitation. DuckDB is an extremely fast in-process analytical database that's easy to use with Python through its API. It provides a simple interface for processing CSV files and offers powerful local analytics capabilities. Polars is a multi-threaded query engine written in Rust, known for its efficient parallelism and high performance on modern processors. Both DuckDB are easy to use and open source. What Datagrunt Is Not Datagrunt is not an extension of DuckDB or Polars, nor is it designed to handle every data processing task. Its core purpose is to accurately infer CSV delimiters and expose them to classes that offer helper methods for common data tasks. Datagrunt helps load CSV files into Polars dataframes or convert them into new formats. It's not a replacement for DuckDB, Pandas, or Polars. Complete Choice and Freedom Datagrunt doesn't restrict you to the features exposed through its library.. By default, it loads data into Polars dataframes when returning objects in memory, but you can convert them into Pandas dataframes using the to_pandas method. While some users may only need Datagrunt for delimiter inference, it provides helper classes and methods that can be used as desired. You can always leverage Datagrunt's output in other contexts within the same script or application.","title":"Home"},{"location":"#welcome-to-datagrunt","text":"Datagrunt is a lightweight CSV processing library that simplifies handling CSV files and converting them into various formats such as Excel, JSON, JSON Newline Delimited, and Parquet. It eliminates repetitive coding tasks when working with CSVs, empowering data analysts, engineers, and scientists to focus on insights rather than tedious data wrangling.","title":"Welcome to Datagrunt!"},{"location":"#key-features","text":"Intelligent Delimiter Inference: Datagrunt automatically detects and applies the correct delimiter for CSV files using a custom algorithm powered by DuckDB. Seamless Data Processing: Leverage DuckDB and Polars to perform data processing tasks directly on CSV data. Flexible Transformation: Easily convert processed CSV data into different formats to meet your needs. Pythonic API: Enjoy a clean and intuitive API that seamlessly integrates into existing Python workflows.","title":"Key Features"},{"location":"#about-duckdb-and-polars","text":"Processing CSV files, especially large ones locally or in resource-constrained data pipelines, can be challenging. Python, while powerful, is not known for its speed in heavy data processing tasks. DuckDB and Polars address this limitation. DuckDB is an extremely fast in-process analytical database that's easy to use with Python through its API. It provides a simple interface for processing CSV files and offers powerful local analytics capabilities. Polars is a multi-threaded query engine written in Rust, known for its efficient parallelism and high performance on modern processors. Both DuckDB are easy to use and open source.","title":"About DuckDB and Polars"},{"location":"#what-datagrunt-is-not","text":"Datagrunt is not an extension of DuckDB or Polars, nor is it designed to handle every data processing task. Its core purpose is to accurately infer CSV delimiters and expose them to classes that offer helper methods for common data tasks. Datagrunt helps load CSV files into Polars dataframes or convert them into new formats. It's not a replacement for DuckDB, Pandas, or Polars.","title":"What Datagrunt Is Not"},{"location":"#complete-choice-and-freedom","text":"Datagrunt doesn't restrict you to the features exposed through its library.. By default, it loads data into Polars dataframes when returning objects in memory, but you can convert them into Pandas dataframes using the to_pandas method. While some users may only need Datagrunt for delimiter inference, it provides helper classes and methods that can be used as desired. You can always leverage Datagrunt's output in other contexts within the same script or application.","title":"Complete Choice and Freedom"},{"location":"api/","text":"API Documentation Core Classes FileProperties Description : A base class providing essential properties and methods for working with files. Properties filepath (str): The full path to the file. filename (str): The name of the file, without the directory path. extension (str): The file extension, including the leading dot (e.g., \".csv\"). extension_string (str): The file extension without the leading dot (e.g., \"csv\"). size_in_bytes (int): The size of the file in bytes. size_in_kb (float): The size of the file in kilobytes. size_in_mb (float): The size of the file in megabytes. size_in_gb (float): The size of the file in gigabytes. size_in_tb (float): The size of the file in terabytes. is_structured (bool): True if the file is considered structured (based on extension). is_semi_structured (bool): True if the file is considered semi-structured (based on extension). is_unstructured (bool): True if the file is considered unstructured (based on extension). is_standard (bool): True if the file is a standard file type (based on extension). is_proprietary (bool): True if the file is a proprietary file type (based on extension). is_csv (bool): True if the file is a CSV file (based on extension). is_excel (bool): True if the file is an Excel file (based on extension). is_apache (bool): True if the file is an Apache file (like Parquet or Avro) (based on extension). is_empty (bool): True if the file size is 0 bytes. is_large (bool): True if the file size is 1 GB or larger. is_tabular (bool): True if the file is a tabular data format (CSV, Excel, TSV). CSVProperties Description: Extends FileProperties with properties and methods specific to CSV files. Properties first_row (str): The content of the first row in the CSV file. delimiter (str): The delimiter used to separate values in the CSV file (inferred). row_count_with_header (int): The total number of rows in the CSV file, including the header row. row_count_without_header (int): The total number of rows in the CSV file, excluding the header row. columns (list): A list of column names from the CSV file's header. columns_string (str): A comma-separated string of column names. columns_byte_string (bytes): A byte string representation of the comma-separated column names. column_count (int): The number of columns in the CSV file. quotechar (str): The character used for quoting fields in the CSV file. escapechar (str): The character used for escaping characters in the CSV file. newline_delimiter (str): The character(s) used to represent a newline in the CSV file. CSV Reader and Writer Classes Properties All of the CSVReader and CSVWriter classes in this section extends the CSVProperties class. CSVReaderDuckDBEngine Description: Reads CSV files and converts them into various in-memory data objects using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVReaderDuckDBEngine class. Instance Methods get_sample() Description: Prints a sample of the CSV file to the console, providing a glimpse into its contents and structure. Parameters: None Returns: None to_dataframe() Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data. to_arrow_table() Description: Reads the CSV file and converts it into a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data. to_dicts() Description: Reads the CSV file and converts it into a list of dictionaries, where each dictionary represents a row in the CSV. Parameters: None Returns: A list of dictionaries representing the CSV data. CSVReaderPolarsEngine Description: Reads CSV files and converts them into various in-memory data objects using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted. get_sample() Description: Reads the CSV file and returns a sample DataFrame with a limited number of rows (defined by DATAFRAME_SAMPLE_ROWS). Parameters: None Returns: A Polars DataFrame containing a sample of the CSV data. to_dataframe() Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data. to_arrow_table() Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data. to_dicts() Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a list of dictionaries. Parameters: None Returns: A list of dictionaries representing the CSV data. CSVWriterDuckDBEngine Description: Converts CSV files to various other supported file types using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVWriterDuckDBEngine class. Instance Methods write_csv() Description: Reads the CSV file using DuckDB and writes it to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used. write_excel() Description: Reads the CSV file using DuckDB and writes it to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used. write_json() Description: Reads the CSV file using DuckDB and writes it to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used. write_json_newline_delimited() Description: Reads the CSV file using DuckDB and writes it to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used. write_parquet() Description: Reads the CSV file using DuckDB and writes it to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used. CSVWriterPolarsEngine Description: Writes data from various sources to CSV files using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Instance Methods write_csv() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used. write_excel() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used. write_json() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used. write_json_newline_delimited() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used. write_parquet() Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used.","title":"API"},{"location":"api/#api-documentation","text":"","title":"API Documentation"},{"location":"api/#core-classes","text":"","title":"Core Classes"},{"location":"api/#fileproperties","text":"Description : A base class providing essential properties and methods for working with files. Properties filepath (str): The full path to the file. filename (str): The name of the file, without the directory path. extension (str): The file extension, including the leading dot (e.g., \".csv\"). extension_string (str): The file extension without the leading dot (e.g., \"csv\"). size_in_bytes (int): The size of the file in bytes. size_in_kb (float): The size of the file in kilobytes. size_in_mb (float): The size of the file in megabytes. size_in_gb (float): The size of the file in gigabytes. size_in_tb (float): The size of the file in terabytes. is_structured (bool): True if the file is considered structured (based on extension). is_semi_structured (bool): True if the file is considered semi-structured (based on extension). is_unstructured (bool): True if the file is considered unstructured (based on extension). is_standard (bool): True if the file is a standard file type (based on extension). is_proprietary (bool): True if the file is a proprietary file type (based on extension). is_csv (bool): True if the file is a CSV file (based on extension). is_excel (bool): True if the file is an Excel file (based on extension). is_apache (bool): True if the file is an Apache file (like Parquet or Avro) (based on extension). is_empty (bool): True if the file size is 0 bytes. is_large (bool): True if the file size is 1 GB or larger. is_tabular (bool): True if the file is a tabular data format (CSV, Excel, TSV).","title":"FileProperties"},{"location":"api/#csvproperties","text":"Description: Extends FileProperties with properties and methods specific to CSV files. Properties first_row (str): The content of the first row in the CSV file. delimiter (str): The delimiter used to separate values in the CSV file (inferred). row_count_with_header (int): The total number of rows in the CSV file, including the header row. row_count_without_header (int): The total number of rows in the CSV file, excluding the header row. columns (list): A list of column names from the CSV file's header. columns_string (str): A comma-separated string of column names. columns_byte_string (bytes): A byte string representation of the comma-separated column names. column_count (int): The number of columns in the CSV file. quotechar (str): The character used for quoting fields in the CSV file. escapechar (str): The character used for escaping characters in the CSV file. newline_delimiter (str): The character(s) used to represent a newline in the CSV file.","title":"CSVProperties"},{"location":"api/#csv-reader-and-writer-classes","text":"","title":"CSV Reader and Writer Classes"},{"location":"api/#properties","text":"All of the CSVReader and CSVWriter classes in this section extends the CSVProperties class.","title":"Properties"},{"location":"api/#csvreaderduckdbengine","text":"Description: Reads CSV files and converts them into various in-memory data objects using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVReaderDuckDBEngine class. Instance Methods","title":"CSVReaderDuckDBEngine"},{"location":"api/#get_sample","text":"Description: Prints a sample of the CSV file to the console, providing a glimpse into its contents and structure. Parameters: None Returns: None","title":"get_sample()"},{"location":"api/#to_dataframe","text":"Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data.","title":"to_dataframe()"},{"location":"api/#to_arrow_table","text":"Description: Reads the CSV file and converts it into a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data.","title":"to_arrow_table()"},{"location":"api/#to_dicts","text":"Description: Reads the CSV file and converts it into a list of dictionaries, where each dictionary represents a row in the CSV. Parameters: None Returns: A list of dictionaries representing the CSV data.","title":"to_dicts()"},{"location":"api/#csvreaderpolarsengine","text":"Description: Reads CSV files and converts them into various in-memory data objects using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted.","title":"CSVReaderPolarsEngine"},{"location":"api/#get_sample_1","text":"Description: Reads the CSV file and returns a sample DataFrame with a limited number of rows (defined by DATAFRAME_SAMPLE_ROWS). Parameters: None Returns: A Polars DataFrame containing a sample of the CSV data.","title":"get_sample()"},{"location":"api/#to_dataframe_1","text":"Description: Reads the CSV file and converts it into a Polars DataFrame. Parameters: None Returns: A Polars DataFrame representing the CSV data.","title":"to_dataframe()"},{"location":"api/#to_arrow_table_1","text":"Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a PyArrow Table. Parameters: None Returns: A PyArrow Table representing the CSV data.","title":"to_arrow_table()"},{"location":"api/#to_dicts_1","text":"Description: Reads the CSV file, converts it into a Polars DataFrame, and then converts the DataFrame to a list of dictionaries. Parameters: None Returns: A list of dictionaries representing the CSV data.","title":"to_dicts()"},{"location":"api/#csvwriterduckdbengine","text":"Description: Converts CSV files to various other supported file types using DuckDB. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Description: Initializes the CSVWriterDuckDBEngine class. Instance Methods","title":"CSVWriterDuckDBEngine"},{"location":"api/#write_csv","text":"Description: Reads the CSV file using DuckDB and writes it to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used.","title":"write_csv()"},{"location":"api/#write_excel","text":"Description: Reads the CSV file using DuckDB and writes it to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used.","title":"write_excel()"},{"location":"api/#write_json","text":"Description: Reads the CSV file using DuckDB and writes it to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used.","title":"write_json()"},{"location":"api/#write_json_newline_delimited","text":"Description: Reads the CSV file using DuckDB and writes it to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used.","title":"write_json_newline_delimited()"},{"location":"api/#write_parquet","text":"Description: Reads the CSV file using DuckDB and writes it to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used.","title":"write_parquet()"},{"location":"api/#csvwriterpolarsengine","text":"Description: Writes data from various sources to CSV files using Polars. Class Parameter : filepath (str): The path to the CSV file to be read and converted. Instance Methods","title":"CSVWriterPolarsEngine"},{"location":"api/#write_csv_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a new CSV file. Parameters: out_filename (str, optional): The desired filename for the output CSV file. If not provided, a default filename will be used.","title":"write_csv()"},{"location":"api/#write_excel_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to an Excel file. Parameters: out_filename (str, optional): The desired filename for the output Excel file. If not provided, a default filename will be used.","title":"write_excel()"},{"location":"api/#write_json_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON file. Parameters: out_filename (str, optional): The desired filename for the output JSON file. If not provided, a default filename will be used.","title":"write_json()"},{"location":"api/#write_json_newline_delimited_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a JSON Lines (newline-delimited JSON) file. Parameters: out_filename (str, optional): The desired filename for the output JSON Lines file. If not provided, a default filename will be used.","title":"write_json_newline_delimited()"},{"location":"api/#write_parquet_1","text":"Description: Reads the CSV file, converts it to a Polars DataFrame, and writes the DataFrame to a Parquet file. Parameters: out_filename (str, optional): The desired filename for the output Parquet file. If not provided, a default filename will be used.","title":"write_parquet()"},{"location":"examples/","text":"Usage Examples","title":"Usage Examples"},{"location":"examples/#usage-examples","text":"","title":"Usage Examples"}]}