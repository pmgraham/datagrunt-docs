{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Datagrunt! Datagrunt is a lightweight, open-source Python library designed to streamline CSV file handling and conversion. By automating common tasks, Datagrunt allows data professionals to focus on analysis rather than data wrangling. Key Features Smart Delimiter Detection: Automatically identifies the correct CSV delimiter using a custom DuckDB-powered algorithm. Efficient Data Processing: Utilizes DuckDB and Polars for high-performance CSV data manipulation. Multiple Output Formats: Easily convert CSV data to Excel, JSON, JSON Newline Delimited, and Parquet. User-Friendly API: Integrates seamlessly with existing Python workflows. Why Datagrunt? Processing large CSV files locally or in resource-constrained environments can be challenging. Datagrunt leverages the speed of DuckDB and Polars to overcome Python's limitations in heavy data processing tasks. Power Tools Under the Hood Tool Description DuckDB Fast in-process analytical database with a simple Python API Polars Multi-threaded query engine written in Rust, optimized for modern processors Datagrunt's Role Datagrunt is not an extension of DuckDB or Polars, nor a comprehensive data processing solution. Its primary functions are: Accurately inferring CSV delimiters Providing helper methods for common data tasks Facilitating CSV file loading into Polars dataframes Enabling conversion to various output formats Flexibility and Integration While Datagrunt uses Polars dataframes by default, it doesn't limit your options: Convert to Pandas dataframes using the to_pandas method Use Datagrunt's output in other contexts within your script or application Leverage only specific features (e.g., delimiter inference) as needed Datagrunt complements your existing data processing toolkit, offering helpful utilities without restricting your workflow choices.","title":"Home"},{"location":"#welcome-to-datagrunt","text":"Datagrunt is a lightweight, open-source Python library designed to streamline CSV file handling and conversion. By automating common tasks, Datagrunt allows data professionals to focus on analysis rather than data wrangling.","title":"Welcome to Datagrunt!"},{"location":"#key-features","text":"Smart Delimiter Detection: Automatically identifies the correct CSV delimiter using a custom DuckDB-powered algorithm. Efficient Data Processing: Utilizes DuckDB and Polars for high-performance CSV data manipulation. Multiple Output Formats: Easily convert CSV data to Excel, JSON, JSON Newline Delimited, and Parquet. User-Friendly API: Integrates seamlessly with existing Python workflows.","title":"Key Features"},{"location":"#why-datagrunt","text":"Processing large CSV files locally or in resource-constrained environments can be challenging. Datagrunt leverages the speed of DuckDB and Polars to overcome Python's limitations in heavy data processing tasks.","title":"Why Datagrunt?"},{"location":"#power-tools-under-the-hood","text":"Tool Description DuckDB Fast in-process analytical database with a simple Python API Polars Multi-threaded query engine written in Rust, optimized for modern processors","title":"Power Tools Under the Hood"},{"location":"#datagrunts-role","text":"Datagrunt is not an extension of DuckDB or Polars, nor a comprehensive data processing solution. Its primary functions are: Accurately inferring CSV delimiters Providing helper methods for common data tasks Facilitating CSV file loading into Polars dataframes Enabling conversion to various output formats","title":"Datagrunt's Role"},{"location":"#flexibility-and-integration","text":"While Datagrunt uses Polars dataframes by default, it doesn't limit your options: Convert to Pandas dataframes using the to_pandas method Use Datagrunt's output in other contexts within your script or application Leverage only specific features (e.g., delimiter inference) as needed Datagrunt complements your existing data processing toolkit, offering helpful utilities without restricting your workflow choices.","title":"Flexibility and Integration"},{"location":"api/","text":"API Reference Class Hierarchy Here is the hierarchy of classes for Datagrunt: FileProperties : This class serves as the foundation for defining core file properties for any file type. CSVProperties : Inherits from FileProperties. Adds properties specifically for CSV files. An error is raised if any file without the .csv file extension. CSVReader : Inherits from CSVProperties and adds helper methods for reading and querying CSV files. CSVWriter : Inherits from CSVProperties and adds helper methods for converting CSV files to various other supported file types. FileProperties The base class for file objects. __init__(filepath) : Initialize the FileProperties with a CSV file path. Properties filepath : Path to the file filename : Name of the file extension : File extension (with dot) extension_string : File extension (without dot) size_in_bytes : File size in bytes size_in_kb : File size in kilobytes size_in_mb : File size in megabytes size_in_gb : File size in gigabytes size_in_tb : File size in terabytes is_structured : Boolean indicating if the file is structured is_semi_structured : Boolean indicating if the file is semi-structured is_unstructured : Boolean indicating if the file is unstructured is_standard : Boolean indicating if the file has a standard format is_proprietary : Boolean indicating if the file has a proprietary format is_csv : Boolean indicating if the file is a CSV is_excel : Boolean indicating if the file is an Excel file is_apache : Boolean indicating if the file is an Apache formatted file is_empty : Boolean indicating if the file is empty is_large : Boolean indicating if the file is larger than or equal to 1 GB is_tabular : Boolean indicating if the file is tabular CSVProperties A subclass of FileProperties, specifically for CSV files. Initialized with a CSV file path. Properties (in addition to FileProperties) delimiter : The delimiter used in the CSV file. Delimiter is automatically inferred and may be optionally re-assigned by the user. row_count_with_header : Number of rows including the header row_count_without_header : Number of rows excluding the header columns : List of column names columns_string : Comma-separated string of column names columns_byte_string : Byte string of comma-separated column names column_count : Number of columns quotechar : Quote character used in the CSV escapechar : Escape character used in the CSV newline_delimiter : Newline delimiter used in the CSV CSVReader A class for reading and querying CSV files. Methods __init__(filepath: str, engine: str = 'polars') -> None Initialize the CSVReader with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for reading ('polars' or 'duckdb'). Defaults to 'polars'. Returns: None get_sample() -> Union[pl.DataFrame, duckdb.DuckDBPyRelation] Return a sample of the CSV file. Args: None Returns: A sample of the data (Polars DataFrame or DuckDB Relation, depending on the engine). to_dataframe() -> pl.DataFrame Convert the CSV to a Polars dataframe. Args: None Returns: A Polars DataFrame containing the CSV data. to_arrow_table() -> pa.Table Convert the CSV to a PyArrow table. Args: None Returns: A PyArrow Table containing the CSV data. to_dicts() -> List[Dict[str, Any]] Convert the CSV to a list of dictionaries. Args: None Returns: A list of dictionaries, where each dictionary represents a row in the CSV. query_data(sql_query: str) -> duckdb.DuckDBPyRelation Query the CSV file using SQL (via DuckDB). Args: sql_query (str): SQL query to execute against the CSV data. Returns: A DuckDB Relation containing the query results. Properties db_table : The name of the table in DuckDB where the CSV data is imported. engine : The engine being used ('polars' or 'duckdb'). CSVWriter A class for converting CSV files to various other supported file types. Methods __init__(filepath: str, engine: str = 'duckdb') -> None Initialize the CSVWriter with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for writing ('polars' or 'duckdb'). Defaults to 'duckdb'. Returns: None write_csv(out_filename: Optional[str] = None) -> str Write the CSV data to a new CSV file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written CSV file. write_excel(out_filename: Optional[str] = None) -> str Write the CSV data to an Excel file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Excel file. write_json(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON file. write_json_newline_delimited(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON newline delimited file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON newline delimited file. write_parquet(out_filename: Optional[str] = None) -> str Write the CSV data to a Parquet file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Parquet file. Properties db_table : The name of the table in DuckDB where the CSV data is imported. engine : The engine being used ('polars' or 'duckdb').","title":"API"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#class-hierarchy","text":"Here is the hierarchy of classes for Datagrunt: FileProperties : This class serves as the foundation for defining core file properties for any file type. CSVProperties : Inherits from FileProperties. Adds properties specifically for CSV files. An error is raised if any file without the .csv file extension. CSVReader : Inherits from CSVProperties and adds helper methods for reading and querying CSV files. CSVWriter : Inherits from CSVProperties and adds helper methods for converting CSV files to various other supported file types.","title":"Class Hierarchy"},{"location":"api/#fileproperties","text":"The base class for file objects. __init__(filepath) : Initialize the FileProperties with a CSV file path.","title":"FileProperties"},{"location":"api/#properties","text":"filepath : Path to the file filename : Name of the file extension : File extension (with dot) extension_string : File extension (without dot) size_in_bytes : File size in bytes size_in_kb : File size in kilobytes size_in_mb : File size in megabytes size_in_gb : File size in gigabytes size_in_tb : File size in terabytes is_structured : Boolean indicating if the file is structured is_semi_structured : Boolean indicating if the file is semi-structured is_unstructured : Boolean indicating if the file is unstructured is_standard : Boolean indicating if the file has a standard format is_proprietary : Boolean indicating if the file has a proprietary format is_csv : Boolean indicating if the file is a CSV is_excel : Boolean indicating if the file is an Excel file is_apache : Boolean indicating if the file is an Apache formatted file is_empty : Boolean indicating if the file is empty is_large : Boolean indicating if the file is larger than or equal to 1 GB is_tabular : Boolean indicating if the file is tabular","title":"Properties"},{"location":"api/#csvproperties","text":"A subclass of FileProperties, specifically for CSV files. Initialized with a CSV file path.","title":"CSVProperties"},{"location":"api/#properties-in-addition-to-fileproperties","text":"delimiter : The delimiter used in the CSV file. Delimiter is automatically inferred and may be optionally re-assigned by the user. row_count_with_header : Number of rows including the header row_count_without_header : Number of rows excluding the header columns : List of column names columns_string : Comma-separated string of column names columns_byte_string : Byte string of comma-separated column names column_count : Number of columns quotechar : Quote character used in the CSV escapechar : Escape character used in the CSV newline_delimiter : Newline delimiter used in the CSV","title":"Properties (in addition to FileProperties)"},{"location":"api/#csvreader","text":"A class for reading and querying CSV files.","title":"CSVReader"},{"location":"api/#methods","text":"__init__(filepath: str, engine: str = 'polars') -> None Initialize the CSVReader with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for reading ('polars' or 'duckdb'). Defaults to 'polars'. Returns: None get_sample() -> Union[pl.DataFrame, duckdb.DuckDBPyRelation] Return a sample of the CSV file. Args: None Returns: A sample of the data (Polars DataFrame or DuckDB Relation, depending on the engine). to_dataframe() -> pl.DataFrame Convert the CSV to a Polars dataframe. Args: None Returns: A Polars DataFrame containing the CSV data. to_arrow_table() -> pa.Table Convert the CSV to a PyArrow table. Args: None Returns: A PyArrow Table containing the CSV data. to_dicts() -> List[Dict[str, Any]] Convert the CSV to a list of dictionaries. Args: None Returns: A list of dictionaries, where each dictionary represents a row in the CSV. query_data(sql_query: str) -> duckdb.DuckDBPyRelation Query the CSV file using SQL (via DuckDB). Args: sql_query (str): SQL query to execute against the CSV data. Returns: A DuckDB Relation containing the query results.","title":"Methods"},{"location":"api/#properties_1","text":"db_table : The name of the table in DuckDB where the CSV data is imported. engine : The engine being used ('polars' or 'duckdb').","title":"Properties"},{"location":"api/#csvwriter","text":"A class for converting CSV files to various other supported file types.","title":"CSVWriter"},{"location":"api/#methods_1","text":"__init__(filepath: str, engine: str = 'duckdb') -> None Initialize the CSVWriter with a file path and optional engine choice. Args: filepath (str): Path to the CSV file. engine (str, optional): Engine to use for writing ('polars' or 'duckdb'). Defaults to 'duckdb'. Returns: None write_csv(out_filename: Optional[str] = None) -> str Write the CSV data to a new CSV file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written CSV file. write_excel(out_filename: Optional[str] = None) -> str Write the CSV data to an Excel file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Excel file. write_json(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON file. write_json_newline_delimited(out_filename: Optional[str] = None) -> str Write the CSV data to a JSON newline delimited file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written JSON newline delimited file. write_parquet(out_filename: Optional[str] = None) -> str Write the CSV data to a Parquet file. Args: out_filename (str, optional): Name of the output file. If None, a default name will be used. Returns: The path of the written Parquet file.","title":"Methods"},{"location":"api/#properties_2","text":"db_table : The name of the table in DuckDB where the CSV data is imported. engine : The engine being used ('polars' or 'duckdb').","title":"Properties"},{"location":"examples/","text":"Usage Examples Datagrunt offers a variety of functionalities. Here are some common use cases to help you get started. Setting The Processing Engine Datagrunt gives you two options for CSV file processing: DuckDB and Polars. You set the engine as follows: from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' dg = CSVReader(csv_file) # by default engine is set to 'polars'. Alternatively, you may set the processing engine to DuckDB by passing the 'duckdb' value for the engine param as follows: from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) The interface is the exact same for both engines. If you write your code using Polars, but want to change to DuckDB later, simply update the engine param when you instantiate the CSVReader class and leave the rest of your code untouched. The engine functionality works exactly the same way for the CSVWriter class as well. Here is an example using the Polars engine: from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' dg = CSVWriter(csv_file) dg.write_json() Here is an example using the DuckDB engine: from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVWriter(csv_file, engine=engine) dg.write_json() Working with CSV Properties Get CSV Delimiter from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) delimiter = csv_props.delimiter # Optional: Use delimiter.encode() to return a bytestring Manually Set CSV Delimiter from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) csv_props.delimiter = ',' # Override the inferred delimiter Sampling CSV Data from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' # Sample using DuckDB (prints to screen automatically) CSVReader(csv_file, engine='duckdb').get_sample() # Sample using Polars (prints to screen automatically) CSVReader(csv_file).get_sample() Reading CSV Files Using DuckDB from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' duckdb_reader = CSVReader(csv_file, engine=engine) df_duckdb = duckdb_reader.to_dataframe() Using Polars from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' polars_reader = CSVReader(csv_file) df_polars = polars_reader.to_dataframe() Writing CSV to Other Formats JSON and JSON Lines with DuckDB from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' duckdb_writer = CSVWriter(csv_file, engine=engine) # Write to JSON duckdb_writer.write_json() # Optional: Specify output filename # Write to JSON Lines duckdb_writer.write_json_newline_delimited() JSON and JSON Lines with Polars from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' polars_writer = CSVWriter(csv_file) # Write to JSON polars_writer.write_json() # Optional: Specify output filename # Write to JSON Lines polars_writer.write_json_newline_delimited() # Use .jsonl extension if specifying filename Integration with Pandas import pandas as pd from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) df_polars = csv_props.to_dataframe() # returns Polars dataframe automatically # convert Polars dataframe to Pandas df_pandas_from_polars = df_polars.to_pandas() # Use Datagrunt to get the correct delimiter for Pandas df_pandas = pd.read_csv(csv_file, delimiter=csv_props.delimiter) Using SQL To Query CSV Files It's recommened to use the duckdb engine when querying CSV files. These query examples will work with the polars engine exactly the same way as presented, but Datagrunt converts all data to strings when using duckdb and it better preserves the integrity of the data due to not coercing into inferred datatypes. You'll see below that you can use SQL and you can optionally cast columns to different data types as needed. For example, you'll notice there are postal codes in the data below. Usually, postal codes are read as integers by most processing engines. Many postal codes begin with a 0. If postal codes are read as an integer, the leading 0 will be dropped erroneously. This can be fixed by using the lpad function in Python, but we'd prefer to present the data in its original form as a string as opposed to having the data altered due to data type conversion. csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() # only a sample output below is represented. VIN (1-10) County City State DOL Vehicle ID Vehicle Location Electric Utility 2020 Census Tract 5YJSA1E28K Snohomish Mukilteo WA 236424583 POINT (-122.29943 47.912654) PUGET SOUND ENERGY INC 53061042001 1C4JJXP68P Yakima Yakima WA 249905295 POINT (-120.468875 46.6046178) PACIFICORP 53077001601 WBY8P6C05L Kitsap Kingston WA 260917289 POINT (-122.5178351 47.7981436) PUGET SOUND ENERGY INC 53035090102 JTDKARFP1J Kitsap Port Orchard WA 186410087 POINT (-122.6530052 47.4739066) PUGET SOUND ENERGY INC 53035092802 5UXTA6C09N Snohomish Everett WA 186076915 POINT (-122.2032349 47.8956271) PUGET SOUND ENERGY INC 53061041605 JTMAB3FVXR Snohomish Snohomish WA 262809249 POINT (-122.0483457 47.9435765) PUGET SOUND ENERGY INC 53061052402 7FCTGAAA7P Pierce Orting WA 252195450 POINT (-122.1977914 47.0948565) PUGET SOUND ENERGY INC||CITY O... 53053070100 1V2GNPE87P Spokane Spokane WA 227314790 POINT (-117.428902 47.658268) MO Write a SQL Query query = f\"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM {dg.db_table} ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 ... ... Vista 1 Tempe 1 Green Bay 1 Waverly 1 Creston 1 764 rows (20 shown) 2 columns Query a Polars Dataframe Note: in the code example below that we are not interpolating the dataframe variable. We are referencing it by embedding it in the string. df = dg.to_dataframe() df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(df_query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 Kirkland 5883 Renton 5835 Sammamish 5795 Olympia 4830 Tacoma 4204 ... ... ... ... ... ... Lamont 1 Dickinson 1 Saratoga Springs 1 Gunpowder 1 Holden Village 1 Yorktown 1 Ridgecrest 1 Startup 1 Sacramento 1 Washtucna 1 764 rows (20 shown) 2 columns Query Results To Dataframe In the previous example we demonstrated querying a Polars dataframe by using SQL. Rather than simply returning results, you may convert the results to any type of dataframe you wish. df = dg.to_dataframe() df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" df_polars = dg.query_data(df_query).pl() # return a Polars dataframe df_pandas = dg.query_data(df_query).to_df() # return a Pandas dataframe Credits and References The SQL query functionality within Datagrunt is powered by DuckDB . The dataset used in these code examples is sourced from Electric Vehicle Population Data .","title":"Usage Examples"},{"location":"examples/#usage-examples","text":"Datagrunt offers a variety of functionalities. Here are some common use cases to help you get started.","title":"Usage Examples"},{"location":"examples/#setting-the-processing-engine","text":"Datagrunt gives you two options for CSV file processing: DuckDB and Polars. You set the engine as follows: from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' dg = CSVReader(csv_file) # by default engine is set to 'polars'. Alternatively, you may set the processing engine to DuckDB by passing the 'duckdb' value for the engine param as follows: from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) The interface is the exact same for both engines. If you write your code using Polars, but want to change to DuckDB later, simply update the engine param when you instantiate the CSVReader class and leave the rest of your code untouched. The engine functionality works exactly the same way for the CSVWriter class as well. Here is an example using the Polars engine: from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' dg = CSVWriter(csv_file) dg.write_json() Here is an example using the DuckDB engine: from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' dg = CSVWriter(csv_file, engine=engine) dg.write_json()","title":"Setting The Processing Engine"},{"location":"examples/#working-with-csv-properties","text":"","title":"Working with CSV Properties"},{"location":"examples/#get-csv-delimiter","text":"from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) delimiter = csv_props.delimiter # Optional: Use delimiter.encode() to return a bytestring","title":"Get CSV Delimiter"},{"location":"examples/#manually-set-csv-delimiter","text":"from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) csv_props.delimiter = ',' # Override the inferred delimiter","title":"Manually Set CSV Delimiter"},{"location":"examples/#sampling-csv-data","text":"from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' # Sample using DuckDB (prints to screen automatically) CSVReader(csv_file, engine='duckdb').get_sample() # Sample using Polars (prints to screen automatically) CSVReader(csv_file).get_sample()","title":"Sampling CSV Data"},{"location":"examples/#reading-csv-files","text":"","title":"Reading CSV Files"},{"location":"examples/#using-duckdb","text":"from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' engine = 'duckdb' duckdb_reader = CSVReader(csv_file, engine=engine) df_duckdb = duckdb_reader.to_dataframe()","title":"Using DuckDB"},{"location":"examples/#using-polars","text":"from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' polars_reader = CSVReader(csv_file) df_polars = polars_reader.to_dataframe()","title":"Using Polars"},{"location":"examples/#writing-csv-to-other-formats","text":"","title":"Writing CSV to Other Formats"},{"location":"examples/#json-and-json-lines-with-duckdb","text":"from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' engine = 'duckdb' duckdb_writer = CSVWriter(csv_file, engine=engine) # Write to JSON duckdb_writer.write_json() # Optional: Specify output filename # Write to JSON Lines duckdb_writer.write_json_newline_delimited()","title":"JSON and JSON Lines with DuckDB"},{"location":"examples/#json-and-json-lines-with-polars","text":"from datagrunt.csvfile import CSVWriter csv_file = 'myfile.csv' polars_writer = CSVWriter(csv_file) # Write to JSON polars_writer.write_json() # Optional: Specify output filename # Write to JSON Lines polars_writer.write_json_newline_delimited() # Use .jsonl extension if specifying filename","title":"JSON and JSON Lines with Polars"},{"location":"examples/#integration-with-pandas","text":"import pandas as pd from datagrunt.csvfile import CSVReader csv_file = 'myfile.csv' csv_props = CSVReader(csv_file) df_polars = csv_props.to_dataframe() # returns Polars dataframe automatically # convert Polars dataframe to Pandas df_pandas_from_polars = df_polars.to_pandas() # Use Datagrunt to get the correct delimiter for Pandas df_pandas = pd.read_csv(csv_file, delimiter=csv_props.delimiter)","title":"Integration with Pandas"},{"location":"examples/#using-sql-to-query-csv-files","text":"It's recommened to use the duckdb engine when querying CSV files. These query examples will work with the polars engine exactly the same way as presented, but Datagrunt converts all data to strings when using duckdb and it better preserves the integrity of the data due to not coercing into inferred datatypes. You'll see below that you can use SQL and you can optionally cast columns to different data types as needed. For example, you'll notice there are postal codes in the data below. Usually, postal codes are read as integers by most processing engines. Many postal codes begin with a 0. If postal codes are read as an integer, the leading 0 will be dropped erroneously. This can be fixed by using the lpad function in Python, but we'd prefer to present the data in its original form as a string as opposed to having the data altered due to data type conversion. csv_file = 'electric_vehicle_population_data.csv' engine = 'duckdb' dg = CSVReader(csv_file, engine=engine) # set duckdb as the processing engine. df = dg.to_dataframe() # only a sample output below is represented. VIN (1-10) County City State DOL Vehicle ID Vehicle Location Electric Utility 2020 Census Tract 5YJSA1E28K Snohomish Mukilteo WA 236424583 POINT (-122.29943 47.912654) PUGET SOUND ENERGY INC 53061042001 1C4JJXP68P Yakima Yakima WA 249905295 POINT (-120.468875 46.6046178) PACIFICORP 53077001601 WBY8P6C05L Kitsap Kingston WA 260917289 POINT (-122.5178351 47.7981436) PUGET SOUND ENERGY INC 53035090102 JTDKARFP1J Kitsap Port Orchard WA 186410087 POINT (-122.6530052 47.4739066) PUGET SOUND ENERGY INC 53035092802 5UXTA6C09N Snohomish Everett WA 186076915 POINT (-122.2032349 47.8956271) PUGET SOUND ENERGY INC 53061041605 JTMAB3FVXR Snohomish Snohomish WA 262809249 POINT (-122.0483457 47.9435765) PUGET SOUND ENERGY INC 53061052402 7FCTGAAA7P Pierce Orting WA 252195450 POINT (-122.1977914 47.0948565) PUGET SOUND ENERGY INC||CITY O... 53053070100 1V2GNPE87P Spokane Spokane WA 227314790 POINT (-117.428902 47.658268) MO","title":"Using SQL To Query CSV Files"},{"location":"examples/#write-a-sql-query","text":"query = f\"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM {dg.db_table} ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 ... ... Vista 1 Tempe 1 Green Bay 1 Waverly 1 Creston 1 764 rows (20 shown) 2 columns","title":"Write a SQL Query"},{"location":"examples/#query-a-polars-dataframe","text":"Note: in the code example below that we are not interpolating the dataframe variable. We are referencing it by embedding it in the string. df = dg.to_dataframe() df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" dg.query_data(df_query) city vehicle_count Seattle 32602 Bellevue 9960 Redmond 7165 Vancouver 7081 Bothell 6602 Kirkland 5883 Renton 5835 Sammamish 5795 Olympia 4830 Tacoma 4204 ... ... ... ... ... ... Lamont 1 Dickinson 1 Saratoga Springs 1 Gunpowder 1 Holden Village 1 Yorktown 1 Ridgecrest 1 Startup 1 Sacramento 1 Washtucna 1 764 rows (20 shown) 2 columns","title":"Query a Polars Dataframe"},{"location":"examples/#query-results-to-dataframe","text":"In the previous example we demonstrated querying a Polars dataframe by using SQL. Rather than simply returning results, you may convert the results to any type of dataframe you wish. df = dg.to_dataframe() df_query = \"\"\" WITH core AS ( SELECT City AS city, \"VIN (1-10)\" AS vin FROM df ) SELECT city, COUNT(vin) AS vehicle_count FROM core GROUP BY 1 ORDER BY 2 DESC \"\"\" df_polars = dg.query_data(df_query).pl() # return a Polars dataframe df_pandas = dg.query_data(df_query).to_df() # return a Pandas dataframe","title":"Query Results To Dataframe"},{"location":"examples/#credits-and-references","text":"The SQL query functionality within Datagrunt is powered by DuckDB . The dataset used in these code examples is sourced from Electric Vehicle Population Data .","title":"Credits and References"}]}